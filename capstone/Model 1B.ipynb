{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07404d19",
   "metadata": {},
   "source": [
    "# Generative Model for Wine Tasting Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0bcb69",
   "metadata": {},
   "source": [
    "Wine tasting notes are often viewed as an invaluable part of the wine purchasing decision. Wine tasting notes by professionals, such as Wine Spectator and Wine Advocate, are often printed and used by commercial retailers as an immutable guide to wines sold.\n",
    "\n",
    "However, objectivity in these notes can often be difficult due to expression of only 1/3 of nose receptor phenotypes in humans, leading to vast differences in tasting experience. Furthermore, tasting events often contain 100s, or 1000s, or wines, which can cause fatigue in even the most experienced of tasters.\n",
    "\n",
    "Finally, tasting is often confounded by the lack of a common language, and indeed, a common cultural background against which to compare aromas and smells.\n",
    "\n",
    "The WSET has produced a <a href='https://www.wsetglobal.com/media/3119/wset_l3_wines_sat_en_jun-2016.pdf'>tasting rubric</a>, in an attempt to create a common wine vocabulary, quantifying not just aroma but body and taste, as well.\n",
    "\n",
    "In 2002, A.C. Noble created a wine wheel during her time at UC DAVIS. This wine wheel was revolutionary, not because it created a common language, but because it came with instructions to create a reproducible standard for each aromatic note from common supermarket ingredients.\n",
    "\n",
    "<img src=\"img/Davis-Wine-Aroma-Wheel1.jpg\" alt=\"wine_wheel\" width=\"300\"/>\n",
    "\n",
    "My project will examine 1000s of tastings notes created for top 100 rated wines from Wine Spectator to determine if there is a common wine vocabulary and whether a deep-learning model can be trained to generate appropriate looking wine-tasting notes based on wine type, with reasonable accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d505f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.data.experimental import AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67e515e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7c71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = 'wine_df_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f86a2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv(path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff0c3bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49064, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81c1ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine_df = pd.read_csv(path_name)\n",
    "# wine_notes = [note for note in wine_df['wine_notes'][:10000]]\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(wine_notes)\n",
    "# tensor = tokenizer.texts_to_sequences(wine_notes)\n",
    "# input_sequences = []\n",
    "# for line in tensor:\n",
    "#     for i in range(1, len(line)):\n",
    "#         n_gram_sequence = line[:i+1]\n",
    "#         input_sequences.append(n_gram_sequence)\n",
    "# xs, ys = input_sequences[::-1], input_sequences[-1]\n",
    "# xs = tf.keras.preprocessing.sequence.pad_sequences(xs, maxlen=74, padding='pre')\n",
    "# xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f20edc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Generator\n",
    "\n",
    "class Wine_Generator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, path, num_examples, batch_size):\n",
    "        self.wine_tokenizer = None\n",
    "        self.batch_size = batch_size\n",
    "        self.num_examples = num_examples\n",
    "        self.data = pd.read_csv(path)\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(wine_df[:self.num_examples]) / self.batch_size)).astype(np.int)\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        # path : path to wine_notes.txt file\n",
    "        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
    "        wine_notes = [self.preprocess_sentence(note) for note in self.data['wine_notes'][:self.num_examples]]   \n",
    "        return wine_notes\n",
    "    \n",
    "    def preprocess_sentence(self, w):\n",
    "        w = re.sub(r'(importer: )[\\s\\S]+', \"\", w)\n",
    "        w = re.sub(r'(tel. )(\\(\\d+\\))\\s\\d+\\-\\d+[,;]?', \"\", w)\n",
    "        w = re.sub(r'(www.)\\w+.\\w+', \"\",w)\n",
    "        # w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "    \n",
    "    def tokenize(self):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.data)\n",
    "        return tokenizer\n",
    "    \n",
    "    def get_tokenizer(self):\n",
    "        self.data = self.create_dataset()\n",
    "        self.wine_tokenizer = self.tokenize()\n",
    "        return self.wine_tokenizer\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        wine_notes = self.wine_tokenizer.texts_to_sequences(self.data)\n",
    "        batch = wine_notes[index * self.batch_size : (index+1) * self.batch_size]\n",
    "        \n",
    "        input_sequences = []\n",
    "        for line in batch:\n",
    "            for i in range(1, len(line)):\n",
    "                n_gram_sequence = line[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        sentence, labels = input_sequences[::-1], input_sequences[-1]\n",
    "        \n",
    "        padded_sentence = tf.keras.preprocessing.sequence.pad_sequences(sentence, maxlen=75, padding='pre')\n",
    "\n",
    "        return np.array(padded_sentence), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c8acfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data\n",
    "\n",
    "class WinenoteDataset:\n",
    "    def __init__(self):\n",
    "        self.wine_tokenizer = None\n",
    "\n",
    "    def preprocess_sentence(self, w):\n",
    "        w = re.sub(r'(importer: )[\\s\\S]+', \"\", w)\n",
    "        w = re.sub(r'(tel. )(\\(\\d+\\))\\s\\d+\\-\\d+[,;]?', \"\", w)\n",
    "        w = re.sub(r'(www.)\\w+.\\w+', \"\",w)\n",
    "        # w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    def create_dataset(self, path, num_examples):\n",
    "        # path : path to wine_notes.txt file\n",
    "        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
    "        wine_df = pd.read_csv(path)\n",
    "        wine_notes = [note for note in wine_df['wine_notes'][:num_examples]]\n",
    "        return wine_notes\n",
    "\n",
    "    def tokenize(self, notes):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(notes)\n",
    "        tensor = tokenizer.texts_to_sequences(notes)\n",
    "        padded_tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=75, padding='pre')\n",
    "\n",
    "        return padded_tensor, tokenizer\n",
    "\n",
    "    def load_dataset(self, path, num_examples=None):\n",
    "        wine_notes = self.create_dataset(path, num_examples)\n",
    "        wine_notes, self.wine_tokenizer = self.tokenize(wine_notes)\n",
    "\n",
    "        return wine_notes, self.wine_tokenizer\n",
    "\n",
    "    def call(self, num_examples, BATCH_SIZE):\n",
    "        tensor, self.wine_tokenizer = self.load_dataset(path_name, num_examples)\n",
    "        wine_dataset = tf.data.Dataset.from_tensor_slices(tensor)\n",
    "        wine_dataset = wine_dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "        \n",
    "        return wine_dataset, self.wine_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5f2e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq gen\n",
    "batch_size = 512\n",
    "# Limiting the training examples for faster training\n",
    "num_examples = 10000\n",
    "path = 'wine_df_cleaned.csv'\n",
    "\n",
    "wine_seq_generator = Wine_Generator(path, num_examples, batch_size)\n",
    "wine_tokenizer = wine_seq_generator.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e53f89f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[14869,23] = 7901 is not in [0, 3260)\n\t [[node sequential/embedding/embedding_lookup (defined at \\AppData\\Local\\Temp/ipykernel_25408/3408629741.py:7) ]] [Op:__inference_train_function_10516]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential/embedding/embedding_lookup:\n sequential/embedding/embedding_lookup/6431 (defined at \\anaconda3\\envs\\tensorflow\\lib\\contextlib.py:112)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25408/3408629741.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_examples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#     callbacks = [earlystop, checkpoints]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  indices[14869,23] = 7901 is not in [0, 3260)\n\t [[node sequential/embedding/embedding_lookup (defined at \\AppData\\Local\\Temp/ipykernel_25408/3408629741.py:7) ]] [Op:__inference_train_function_10516]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential/embedding/embedding_lookup:\n sequential/embedding/embedding_lookup/6431 (defined at \\anaconda3\\envs\\tensorflow\\lib\\contextlib.py:112)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# compile model first\n",
    "\n",
    "model.fit(\n",
    "    wine_seq_generator,\n",
    "    steps_per_epoch = (num_examples // batch_size),\n",
    "    epochs = 5,\n",
    "    verbose = 1,\n",
    "#     callbacks = [earlystop, checkpoints] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19525e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9252f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# td.data\n",
    "batch_size = 256\n",
    "# Limiting the training examples for faster training\n",
    "num_examples = 2000\n",
    "\n",
    "dataset_creator = WinenoteDataset()\n",
    "wine_dataset, wine_tokenizer = dataset_creator.call(num_examples, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23ba2c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 75), dtype=int32, numpy=\n",
       "array([[   0,    0,    0, ..., 4082,   83,  235],\n",
       "       [   0,    0,    0, ..., 4083,   83,   65],\n",
       "       [   0,    0,    0, ..., 1868,   83,  235],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  847, 1168, 1169],\n",
       "       [   0,    0,    0, ...,  383,  237,  414],\n",
       "       [   0,    0,    0, ...,  182,  237,  414]])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch = next(iter(wine_dataset))\n",
    "example_input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b65e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 74, 250)           1784750   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 74, 300)           481200    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 300)               541200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7139)              2148839   \n",
      "=================================================================\n",
      "Total params: 4,955,989\n",
      "Trainable params: 4,955,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "max_len = 75         # length of output sequence \n",
    "total_words = len(wine_tokenizer.index_word)+1\n",
    "\n",
    "num_train_steps = (num_examples // batch_size + 1) * n_epochs    # len(tokenized_dataset['input_ids']) // batch_size * n_epochs\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5,\n",
    "    end_learning_rate=0.,\n",
    "    decay_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 250, input_length=max_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(learning_rate=0.01)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "checkpoint_path = \"/model_1/checkpoints/model_1.cpkt\"\n",
    "\n",
    "earlystop = EarlyStopping(monitor='sparse_categorical_accuracy', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "checkpoints = ModelCheckpoint(\n",
    "    filepath=checkpoint_path, monitor='sparse_categorical_accuracy', verbose=1, save_best_only=False,\n",
    "    save_weights_only=False, mode='auto', save_freq='epoch',\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e64fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sequences = []\n",
    "# for wine_notes in wine_dataset.take(steps_per_epoch):\n",
    "#     for line in wine_notes.numpy():\n",
    "#         for i in range(1, len(line)):\n",
    "#             if line[i] != 0:\n",
    "#                 n_gram_sequence = line[:i+1]\n",
    "#                 input_sequences.append(n_gram_sequence)\n",
    "# input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=75, padding='pre')\n",
    "# sentences, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "# print(sentences.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea61f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - ETA: 0s - loss: 6.7438 - sparse_categorical_accuracy: 0.0609\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "54/54 [==============================] - 240s 4s/step - loss: 6.7438 - sparse_categorical_accuracy: 0.0609\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.3627 - sparse_categorical_accuracy: 0.0741\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "58/58 [==============================] - 325s 6s/step - loss: 6.3627 - sparse_categorical_accuracy: 0.0741\n",
      "51/51 [==============================] - ETA: 0s - loss: 6.1517 - sparse_categorical_accuracy: 0.1098\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "51/51 [==============================] - 353s 7s/step - loss: 6.1517 - sparse_categorical_accuracy: 0.1098\n",
      "54/54 [==============================] - ETA: 0s - loss: 6.0911 - sparse_categorical_accuracy: 0.1359\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "54/54 [==============================] - 311s 6s/step - loss: 6.0911 - sparse_categorical_accuracy: 0.1359\n",
      "57/57 [==============================] - ETA: 0s - loss: 5.7572 - sparse_categorical_accuracy: 0.1809\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "57/57 [==============================] - 395s 7s/step - loss: 5.7572 - sparse_categorical_accuracy: 0.1809\n",
      "51/51 [==============================] - ETA: 0s - loss: 5.8218 - sparse_categorical_accuracy: 0.1700\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "51/51 [==============================] - 306s 6s/step - loss: 5.8218 - sparse_categorical_accuracy: 0.1700\n",
      "55/55 [==============================] - ETA: 0s - loss: 5.4157 - sparse_categorical_accuracy: 0.1990\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "55/55 [==============================] - 404s 7s/step - loss: 5.4157 - sparse_categorical_accuracy: 0.1990\n",
      "54/54 [==============================] - ETA: 0s - loss: 5.6753 - sparse_categorical_accuracy: 0.1710\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "54/54 [==============================] - 312s 6s/step - loss: 5.6753 - sparse_categorical_accuracy: 0.1710\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.2045 - sparse_categorical_accuracy: 0.1812\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "58/58 [==============================] - 382s 7s/step - loss: 5.2045 - sparse_categorical_accuracy: 0.1812\n",
      "51/51 [==============================] - ETA: 0s - loss: 5.3462 - sparse_categorical_accuracy: 0.1909\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "51/51 [==============================] - 333s 7s/step - loss: 5.3462 - sparse_categorical_accuracy: 0.1909\n",
      "54/54 [==============================] - ETA: 0s - loss: 5.5025 - sparse_categorical_accuracy: 0.1870\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "54/54 [==============================] - 418s 8s/step - loss: 5.5025 - sparse_categorical_accuracy: 0.1870\n",
      "57/57 [==============================] - ETA: 0s - loss: 5.1300 - sparse_categorical_accuracy: 0.2400\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "57/57 [==============================] - 349s 6s/step - loss: 5.1300 - sparse_categorical_accuracy: 0.2400\n",
      "51/51 [==============================] - ETA: 0s - loss: 5.2641 - sparse_categorical_accuracy: 0.2115\n",
      "Epoch 00001: saving model to /model_1/checkpoints\\model_1.cpkt\n",
      "INFO:tensorflow:Assets written to: /model_1/checkpoints\\model_1.cpkt\\assets\n",
      "51/51 [==============================] - 389s 8s/step - loss: 5.2641 - sparse_categorical_accuracy: 0.2115\n",
      " 6/55 [==>...........................] - ETA: 4:25 - loss: 5.2266 - sparse_categorical_accuracy: 0.2350"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = num_examples//batch_size\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for wine_notes in wine_dataset.take(steps_per_epoch):\n",
    "        input_sequences = []\n",
    "        for line in wine_notes.numpy():\n",
    "            for i in range(1, len(line)):\n",
    "                if line[i] != 0:\n",
    "                    n_gram_sequence = line[:i+1]\n",
    "                    input_sequences.append(n_gram_sequence)\n",
    "        input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=75, padding='pre')\n",
    "        sentences, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "        model.fit(\n",
    "            sentences, \n",
    "            labels, \n",
    "            verbose=1, \n",
    "            callbacks = [earlystop, checkpoints], \n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3200c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7aa62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(model, string):\n",
    "    plt.plot(model.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ebe04bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9068/156545278.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_graphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9068/3864618739.py\u001b[0m in \u001b[0;36mplot_graphs\u001b[1;34m(history, string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epochs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "plot_graphs(model, 'accuracy')\n",
    "plot_graphs(model, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d6b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tasting_note(seed_text, length):\n",
    "    last_word = \" \"\n",
    "    for _ in range(length):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        output_word = \"\"         \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        if output_word == last_word:\n",
    "            break\n",
    "        seed_text += \" \" + output_word\n",
    "        last_word = output_word\n",
    "    print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b872c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tasting_note('Aromatic and beautiful', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasting_note('Lush', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasting_note('Red Wine', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasting_note('Wine', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f41ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasting_note('Keyboard', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8922a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasting_note('Bukit Pasoh', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44567471",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_1A.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.models.load_model('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb387d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9732e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
