{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iewo9wS_9id6"
   },
   "source": [
    "# Model 5\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "Model 4 is a model using multi-attention head layers to try to use attention instead of LSTM layers. The model basically encompasses only the Encoder block of a transformer model, due to the language generation model using a y that is the next word.\n",
    "\n",
    "This is different from a model that would need to pay attention to the structure of the y as well, such as a question and answer model, or a translator model.\n",
    "\n",
    "This model is trained on the entire Data Set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kTmK1w5r7SlU",
    "outputId": "f35044b1-f7e2-47f9-cca2-f5ea23842be8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 50.7 MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 60.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 77.4 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.1.1-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 9.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.1.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-_i0ocp1v\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-_i0ocp1v\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.0.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3101540 sha256=13e69b56a66a4c81083195e38b3813286f3a93303feff5c17a4f5861420ac72d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nu13ogc7/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.12.3\n",
      "    Uninstalling transformers-4.12.3:\n",
      "      Successfully uninstalled transformers-4.12.3\n",
      "Successfully installed transformers-4.13.0.dev0\n",
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 88571, done.\u001b[K\n",
      "remote: Counting objects: 100% (256/256), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 88571 (delta 255), reused 253 (delta 253), pack-reused 88315\u001b[K\n",
      "Receiving objects: 100% (88571/88571), 71.29 MiB | 39.27 MiB/s, done.\n",
      "Resolving deltas: 100% (63806/63806), done.\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
      "\u001b[K     |████████████████████████████████| 290 kB 5.3 MB/s \n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 68.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 92.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.1.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 97.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 88.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.7)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 87.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 95.3 MB/s \n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n",
      "Successfully installed aiohttp-3.8.0 aiosignal-1.2.0 async-timeout-4.0.0 asynctest-0.13.0 datasets-1.15.1 frozenlist-1.2.0 fsspec-2021.11.0 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install git+https://github.com/huggingface/transformers # latest version of transformers\n",
    "!git clone https://github.com/huggingface/transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYQXW8xEuJnh",
    "outputId": "e4a26801-bc43-4694-f471-82babac8daa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Rouge) (1.15.0)\n",
      "Installing collected packages: Rouge\n",
      "Successfully installed Rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D601fFiVYQ-d"
   },
   "outputs": [],
   "source": [
    "# Transformers Packages\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "\n",
    "# Tensorflow Packages\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "WVL57it6Fj8f"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import statistics\n",
    "import string\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7QYYRZBRrDq"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9jMDDafHRpm4"
   },
   "outputs": [],
   "source": [
    "def fetch_seq(value):\n",
    "    return test_table['start_seq'][value].values\n",
    "\n",
    "def random_num_pick(value = 1):\n",
    "    return np.random.randint(len(test_table), size = value)\n",
    "\n",
    "def random_text_generator():\n",
    "    pick_value = random_num_pick()\n",
    "    start_seq = fetch_seq(pick_value)[0]\n",
    "\n",
    "    x = generate_messages(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        start_seq,\n",
    "        stop_token,\n",
    "        length = 75,\n",
    "        num_return_sequences = 1,\n",
    "        temperature = 0.7,\n",
    "        k=50,\n",
    "        p=0.9,\n",
    "        repetition_penalty = 1.0,\n",
    "        test_sequences = True\n",
    "    )\n",
    "\n",
    "    end_seq = test_table['end_seq'][pick_value].values[0]\n",
    "\n",
    "    print(f'Starter seed: \"{start_seq}\"')\n",
    "    print('-----------------------------')\n",
    "    print(f'Generated text: \"{x}\"')\n",
    "    print(f'Original tasting note end sequence: {end_seq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading from Google Colabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48sw0TUDFj50",
    "outputId": "ca5b7690-48bb-4730-b93d-c40b143abe1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AUMnh7UiFj3B"
   },
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('/content/drive/MyDrive/DSI24/wine_df_full_cleaned.csv')\n",
    "test_table = pd.read_csv('/content/drive/MyDrive/DSI24/test_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xNj4_8uACL_V"
   },
   "outputs": [],
   "source": [
    "texts = [note for note in wine_df['wine_notes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Zpift3QsCsEa"
   },
   "outputs": [],
   "source": [
    "file_name = 'testing.txt'\n",
    "with open(file_name, 'w') as f:\n",
    "    f.write(\" <|EndOfText|>\\n\".join(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lQ5NV-EC1sv-"
   },
   "outputs": [],
   "source": [
    "cmd = '''\n",
    "python transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
    "    --model_name_or_path distilgpt2 \\\n",
    "    --train_file {0} \\\n",
    "    --do_train \\\n",
    "    --num_train_epochs 20 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --output_dir {1}\n",
    "'''.format(file_name, \"output_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yo2Jj9rk3k1Z",
    "outputId": "19670e5e-a670-44ac-871b-14fa59ee82b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/07/2021 10:02:55 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/07/2021 10:02:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output_full/runs/Nov07_10-02-55_131ce63316b6,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=20.0,\n",
      "output_dir=output_full,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output_full,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/07/2021 10:02:56 - WARNING - datasets.builder - Using custom data configuration default-833549f4955759e0\n",
      "11/07/2021 10:02:56 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
      "100% 1/1 [00:00<00:00, 9489.38it/s]\n",
      "11/07/2021 10:02:56 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "11/07/2021 10:02:56 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "100% 1/1 [00:00<00:00, 1095.69it/s]\n",
      "11/07/2021 10:02:56 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "11/07/2021 10:02:56 - INFO - datasets.builder - Generating split train\n",
      "11/07/2021 10:02:56 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
      "100% 1/1 [00:00<00:00, 808.31it/s]\n",
      "11/07/2021 10:02:57 - WARNING - datasets.builder - Using custom data configuration default-833549f4955759e0\n",
      "11/07/2021 10:02:57 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/07/2021 10:02:57 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
      "11/07/2021 10:02:57 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "11/07/2021 10:02:57 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
      "11/07/2021 10:02:57 - WARNING - datasets.builder - Using custom data configuration default-833549f4955759e0\n",
      "11/07/2021 10:02:57 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/07/2021 10:02:57 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
      "11/07/2021 10:02:57 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "11/07/2021 10:02:57 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
      "11/07/2021 10:02:57 - DEBUG - filelock - Attempting to acquire lock 140493209968528 on /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631.lock\n",
      "11/07/2021 10:02:57 - DEBUG - filelock - Lock 140493209968528 acquired on /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631.lock\n",
      "[INFO|file_utils.py:1753] 2021-11-07 10:02:57,333 >> https://huggingface.co/distilgpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpuu3q5nvr\n",
      "Downloading: 100% 762/762 [00:00<00:00, 1.26MB/s]\n",
      "[INFO|file_utils.py:1757] 2021-11-07 10:02:57,457 >> storing https://huggingface.co/distilgpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "[INFO|file_utils.py:1765] 2021-11-07 10:02:57,457 >> creating metadata file for /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "11/07/2021 10:02:57 - DEBUG - filelock - Attempting to release lock 140493209968528 on /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631.lock\n",
      "11/07/2021 10:02:57 - DEBUG - filelock - Lock 140493209968528 released on /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631.lock\n",
      "[INFO|configuration_utils.py:588] 2021-11-07 10:02:57,457 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "[INFO|configuration_utils.py:625] 2021-11-07 10:02:57,458 >> Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:342] 2021-11-07 10:02:57,590 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:588] 2021-11-07 10:02:57,722 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "[INFO|configuration_utils.py:625] 2021-11-07 10:02:57,723 >> Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "11/07/2021 10:02:57 - DEBUG - filelock - Attempting to acquire lock 140493209746704 on /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n",
      "11/07/2021 10:02:57 - DEBUG - filelock - Lock 140493209746704 acquired on /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n",
      "[INFO|file_utils.py:1753] 2021-11-07 10:02:57,978 >> https://huggingface.co/distilgpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpegspl1tn\n",
      "Downloading: 100% 0.99M/0.99M [00:00<00:00, 6.13MB/s]\n",
      "[INFO|file_utils.py:1757] 2021-11-07 10:02:58,284 >> storing https://huggingface.co/distilgpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|file_utils.py:1765] 2021-11-07 10:02:58,284 >> creating metadata file for /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "11/07/2021 10:02:58 - DEBUG - filelock - Attempting to release lock 140493209746704 on /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n",
      "11/07/2021 10:02:58 - DEBUG - filelock - Lock 140493209746704 released on /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n",
      "11/07/2021 10:02:58 - DEBUG - filelock - Attempting to acquire lock 140493209746704 on /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
      "11/07/2021 10:02:58 - DEBUG - filelock - Lock 140493209746704 acquired on /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
      "[INFO|file_utils.py:1753] 2021-11-07 10:02:58,409 >> https://huggingface.co/distilgpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpk8k7lguo\n",
      "Downloading: 100% 446k/446k [00:00<00:00, 3.10MB/s]\n",
      "[INFO|file_utils.py:1757] 2021-11-07 10:02:58,692 >> storing https://huggingface.co/distilgpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|file_utils.py:1765] 2021-11-07 10:02:58,692 >> creating metadata file for /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "11/07/2021 10:02:58 - DEBUG - filelock - Attempting to release lock 140493209746704 on /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
      "11/07/2021 10:02:58 - DEBUG - filelock - Lock 140493209746704 released on /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
      "11/07/2021 10:02:58 - DEBUG - filelock - Attempting to acquire lock 140493209734544 on /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock\n",
      "11/07/2021 10:02:58 - DEBUG - filelock - Lock 140493209734544 acquired on /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock\n",
      "[INFO|file_utils.py:1753] 2021-11-07 10:02:58,815 >> https://huggingface.co/distilgpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpc881j5ko\n",
      "Downloading: 100% 1.29M/1.29M [00:00<00:00, 9.38MB/s]\n",
      "[INFO|file_utils.py:1757] 2021-11-07 10:02:59,123 >> storing https://huggingface.co/distilgpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|file_utils.py:1765] 2021-11-07 10:02:59,123 >> creating metadata file for /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "11/07/2021 10:02:59 - DEBUG - filelock - Attempting to release lock 140493209734544 on /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock\n",
      "11/07/2021 10:02:59 - DEBUG - filelock - Lock 140493209734544 released on /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-07 10:02:59,510 >> loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-07 10:02:59,510 >> loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-07 10:02:59,510 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-07 10:02:59,510 >> loading file https://huggingface.co/distilgpt2/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-07 10:02:59,510 >> loading file https://huggingface.co/distilgpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-07 10:02:59,510 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:588] 2021-11-07 10:02:59,640 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "[INFO|configuration_utils.py:625] 2021-11-07 10:02:59,641 >> Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "11/07/2021 10:02:59 - DEBUG - filelock - Attempting to acquire lock 140493489573200 on /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba.lock\n",
      "11/07/2021 10:02:59 - DEBUG - filelock - Lock 140493489573200 acquired on /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba.lock\n",
      "[INFO|file_utils.py:1753] 2021-11-07 10:02:59,836 >> https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphctsnwe6\n",
      "Downloading: 100% 336M/336M [00:05<00:00, 67.5MB/s]\n",
      "[INFO|file_utils.py:1757] 2021-11-07 10:03:05,175 >> storing https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
      "[INFO|file_utils.py:1765] 2021-11-07 10:03:05,176 >> creating metadata file for /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
      "11/07/2021 10:03:05 - DEBUG - filelock - Attempting to release lock 140493489573200 on /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba.lock\n",
      "11/07/2021 10:03:05 - DEBUG - filelock - Lock 140493489573200 released on /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba.lock\n",
      "[INFO|modeling_utils.py:1341] 2021-11-07 10:03:05,176 >> loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
      "[INFO|modeling_utils.py:1608] 2021-11-07 10:03:06,366 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1617] 2021-11-07 10:03:06,366 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Running tokenizer on dataset:   0% 0/48 [00:00<?, ?ba/s]11/07/2021 10:03:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-64e0d527d1499f86.arrow\n",
      "Running tokenizer on dataset: 100% 48/48 [00:02<00:00, 17.43ba/s]\n",
      "Running tokenizer on dataset:   0% 0/3 [00:00<?, ?ba/s]11/07/2021 10:03:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-72df9176be54d90a.arrow\n",
      "Running tokenizer on dataset: 100% 3/3 [00:00<00:00, 31.97ba/s]\n",
      "Grouping texts in chunks of 1024:   0% 0/48 [00:00<?, ?ba/s]11/07/2021 10:03:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-49f97cba8c521529.arrow\n",
      "Grouping texts in chunks of 1024: 100% 48/48 [00:28<00:00,  1.69ba/s]\n",
      "Grouping texts in chunks of 1024:   0% 0/3 [00:00<?, ?ba/s]11/07/2021 10:03:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-833549f4955759e0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-e2cc511c6e113600.arrow\n",
      "Grouping texts in chunks of 1024: 100% 3/3 [00:00<00:00,  3.04ba/s]\n",
      "[INFO|trainer.py:1196] 2021-11-07 10:03:50,270 >> ***** Running training *****\n",
      "[INFO|trainer.py:1197] 2021-11-07 10:03:50,270 >>   Num examples = 3932\n",
      "[INFO|trainer.py:1198] 2021-11-07 10:03:50,270 >>   Num Epochs = 20\n",
      "[INFO|trainer.py:1199] 2021-11-07 10:03:50,270 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1200] 2021-11-07 10:03:50,270 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "[INFO|trainer.py:1201] 2021-11-07 10:03:50,270 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1202] 2021-11-07 10:03:50,270 >>   Total optimization steps = 39320\n",
      "{'loss': 2.7245, 'learning_rate': 4.936419125127162e-05, 'epoch': 0.25}\n",
      "  1% 500/39320 [02:01<2:37:50,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:05:52,078 >> Saving model checkpoint to output_full/checkpoint-500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:05:52,079 >> Configuration saved in output_full/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:05:52,636 >> Model weights saved in output_full/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:05:52,637 >> tokenizer config file saved in output_full/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:05:52,637 >> Special tokens file saved in output_full/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 2.3516, 'learning_rate': 4.8728382502543235e-05, 'epoch': 0.51}\n",
      "  3% 1000/39320 [04:05<2:35:52,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:07:55,729 >> Saving model checkpoint to output_full/checkpoint-1000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:07:55,730 >> Configuration saved in output_full/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:07:56,231 >> Model weights saved in output_full/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:07:56,231 >> tokenizer config file saved in output_full/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:07:56,232 >> Special tokens file saved in output_full/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 2.191, 'learning_rate': 4.809257375381486e-05, 'epoch': 0.76}\n",
      "  4% 1500/39320 [06:09<2:33:40,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:09:59,343 >> Saving model checkpoint to output_full/checkpoint-1500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:09:59,344 >> Configuration saved in output_full/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:09:59,847 >> Model weights saved in output_full/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:09:59,847 >> tokenizer config file saved in output_full/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:09:59,847 >> Special tokens file saved in output_full/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 2.1209, 'learning_rate': 4.7456765005086474e-05, 'epoch': 1.02}\n",
      "  5% 2000/39320 [08:12<2:31:42,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:12:02,917 >> Saving model checkpoint to output_full/checkpoint-2000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:12:02,918 >> Configuration saved in output_full/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:12:03,443 >> Model weights saved in output_full/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:12:03,444 >> tokenizer config file saved in output_full/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:12:03,444 >> Special tokens file saved in output_full/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 2.0378, 'learning_rate': 4.682095625635809e-05, 'epoch': 1.27}\n",
      "  6% 2500/39320 [10:16<2:29:51,  4.09it/s][INFO|trainer.py:1995] 2021-11-07 10:14:06,529 >> Saving model checkpoint to output_full/checkpoint-2500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:14:06,530 >> Configuration saved in output_full/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:14:07,029 >> Model weights saved in output_full/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:14:07,030 >> tokenizer config file saved in output_full/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:14:07,030 >> Special tokens file saved in output_full/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.9872, 'learning_rate': 4.6185147507629706e-05, 'epoch': 1.53}\n",
      "  8% 3000/39320 [12:19<2:27:35,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:16:10,111 >> Saving model checkpoint to output_full/checkpoint-3000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:16:10,112 >> Configuration saved in output_full/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:16:10,618 >> Model weights saved in output_full/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:16:10,618 >> tokenizer config file saved in output_full/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:16:10,618 >> Special tokens file saved in output_full/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.9655, 'learning_rate': 4.554933875890132e-05, 'epoch': 1.78}\n",
      "  9% 3500/39320 [14:23<2:25:21,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:18:13,682 >> Saving model checkpoint to output_full/checkpoint-3500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:18:13,683 >> Configuration saved in output_full/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:18:14,185 >> Model weights saved in output_full/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:18:14,186 >> tokenizer config file saved in output_full/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:18:14,186 >> Special tokens file saved in output_full/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 1.9151, 'learning_rate': 4.4913530010172945e-05, 'epoch': 2.03}\n",
      " 10% 4000/39320 [16:26<2:23:32,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:20:17,263 >> Saving model checkpoint to output_full/checkpoint-4000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:20:17,264 >> Configuration saved in output_full/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:20:17,763 >> Model weights saved in output_full/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:20:17,763 >> tokenizer config file saved in output_full/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:20:17,763 >> Special tokens file saved in output_full/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.8757, 'learning_rate': 4.427772126144456e-05, 'epoch': 2.29}\n",
      " 11% 4500/39320 [18:30<2:21:18,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:22:20,805 >> Saving model checkpoint to output_full/checkpoint-4500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:22:20,806 >> Configuration saved in output_full/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:22:21,299 >> Model weights saved in output_full/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:22:21,299 >> tokenizer config file saved in output_full/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:22:21,300 >> Special tokens file saved in output_full/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 1.858, 'learning_rate': 4.364191251271618e-05, 'epoch': 2.54}\n",
      " 13% 5000/39320 [20:34<2:19:23,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:24:24,385 >> Saving model checkpoint to output_full/checkpoint-5000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:24:24,386 >> Configuration saved in output_full/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:24:24,895 >> Model weights saved in output_full/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:24:24,896 >> tokenizer config file saved in output_full/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:24:24,896 >> Special tokens file saved in output_full/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.8534, 'learning_rate': 4.3006103763987794e-05, 'epoch': 2.8}\n",
      " 14% 5500/39320 [22:37<2:17:33,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:26:27,999 >> Saving model checkpoint to output_full/checkpoint-5500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:26:28,000 >> Configuration saved in output_full/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:26:28,514 >> Model weights saved in output_full/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:26:28,515 >> tokenizer config file saved in output_full/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:26:28,515 >> Special tokens file saved in output_full/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 1.8588, 'learning_rate': 4.237029501525942e-05, 'epoch': 3.05}\n",
      " 15% 6000/39320 [24:41<2:15:12,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:28:31,581 >> Saving model checkpoint to output_full/checkpoint-6000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:28:31,582 >> Configuration saved in output_full/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:28:32,082 >> Model weights saved in output_full/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:28:32,083 >> tokenizer config file saved in output_full/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:28:32,083 >> Special tokens file saved in output_full/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.7807, 'learning_rate': 4.173448626653103e-05, 'epoch': 3.31}\n",
      " 17% 6500/39320 [26:44<2:13:13,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:30:35,129 >> Saving model checkpoint to output_full/checkpoint-6500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:30:35,129 >> Configuration saved in output_full/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:30:35,623 >> Model weights saved in output_full/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:30:35,624 >> tokenizer config file saved in output_full/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:30:35,624 >> Special tokens file saved in output_full/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 1.7942, 'learning_rate': 4.109867751780265e-05, 'epoch': 3.56}\n",
      " 18% 7000/39320 [28:48<2:11:20,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:32:38,741 >> Saving model checkpoint to output_full/checkpoint-7000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:32:38,741 >> Configuration saved in output_full/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:32:39,247 >> Model weights saved in output_full/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:32:39,248 >> tokenizer config file saved in output_full/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:32:39,248 >> Special tokens file saved in output_full/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 1.7742, 'learning_rate': 4.0462868769074265e-05, 'epoch': 3.81}\n",
      " 19% 7500/39320 [30:52<2:09:01,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:34:42,324 >> Saving model checkpoint to output_full/checkpoint-7500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:34:42,325 >> Configuration saved in output_full/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:34:42,824 >> Model weights saved in output_full/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:34:42,825 >> tokenizer config file saved in output_full/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:34:42,825 >> Special tokens file saved in output_full/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 1.7673, 'learning_rate': 3.982706002034588e-05, 'epoch': 4.07}\n",
      " 20% 8000/39320 [32:55<2:07:01,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:36:45,866 >> Saving model checkpoint to output_full/checkpoint-8000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:36:45,867 >> Configuration saved in output_full/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:36:46,362 >> Model weights saved in output_full/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:36:46,362 >> tokenizer config file saved in output_full/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:36:46,363 >> Special tokens file saved in output_full/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 1.7525, 'learning_rate': 3.9191251271617504e-05, 'epoch': 4.32}\n",
      " 22% 8500/39320 [34:59<2:05:09,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:38:49,457 >> Saving model checkpoint to output_full/checkpoint-8500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:38:49,458 >> Configuration saved in output_full/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:38:49,951 >> Model weights saved in output_full/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:38:49,951 >> tokenizer config file saved in output_full/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:38:49,951 >> Special tokens file saved in output_full/checkpoint-8500/special_tokens_map.json\n",
      "{'loss': 1.734, 'learning_rate': 3.855544252288912e-05, 'epoch': 4.58}\n",
      " 23% 9000/39320 [37:02<2:03:22,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:40:53,091 >> Saving model checkpoint to output_full/checkpoint-9000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:40:53,092 >> Configuration saved in output_full/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:40:53,589 >> Model weights saved in output_full/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:40:53,589 >> tokenizer config file saved in output_full/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:40:53,589 >> Special tokens file saved in output_full/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 1.7207, 'learning_rate': 3.7919633774160737e-05, 'epoch': 4.83}\n",
      " 24% 9500/39320 [39:06<2:01:00,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:42:56,709 >> Saving model checkpoint to output_full/checkpoint-9500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:42:56,710 >> Configuration saved in output_full/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:42:57,209 >> Model weights saved in output_full/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:42:57,210 >> tokenizer config file saved in output_full/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:42:57,210 >> Special tokens file saved in output_full/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 1.7085, 'learning_rate': 3.728382502543235e-05, 'epoch': 5.09}\n",
      " 25% 10000/39320 [41:09<1:59:09,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:45:00,277 >> Saving model checkpoint to output_full/checkpoint-10000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:45:00,278 >> Configuration saved in output_full/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:45:00,775 >> Model weights saved in output_full/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:45:00,775 >> tokenizer config file saved in output_full/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:45:00,775 >> Special tokens file saved in output_full/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 1.6888, 'learning_rate': 3.664801627670397e-05, 'epoch': 5.34}\n",
      " 27% 10500/39320 [43:13<1:56:56,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:47:03,778 >> Saving model checkpoint to output_full/checkpoint-10500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:47:03,779 >> Configuration saved in output_full/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:47:04,287 >> Model weights saved in output_full/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:47:04,287 >> tokenizer config file saved in output_full/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:47:04,287 >> Special tokens file saved in output_full/checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 1.696, 'learning_rate': 3.601220752797559e-05, 'epoch': 5.6}\n",
      " 28% 11000/39320 [45:17<1:55:09,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:49:07,419 >> Saving model checkpoint to output_full/checkpoint-11000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:49:07,420 >> Configuration saved in output_full/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:49:07,936 >> Model weights saved in output_full/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:49:07,936 >> tokenizer config file saved in output_full/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:49:07,936 >> Special tokens file saved in output_full/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 1.687, 'learning_rate': 3.537639877924721e-05, 'epoch': 5.85}\n",
      " 29% 11500/39320 [47:20<1:52:47,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:51:10,955 >> Saving model checkpoint to output_full/checkpoint-11500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:51:10,956 >> Configuration saved in output_full/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:51:11,527 >> Model weights saved in output_full/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:51:11,528 >> tokenizer config file saved in output_full/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:51:11,528 >> Special tokens file saved in output_full/checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 1.6844, 'learning_rate': 3.4740590030518824e-05, 'epoch': 6.1}\n",
      " 31% 12000/39320 [49:24<1:50:59,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:53:14,619 >> Saving model checkpoint to output_full/checkpoint-12000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:53:14,619 >> Configuration saved in output_full/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:53:15,187 >> Model weights saved in output_full/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:53:15,188 >> tokenizer config file saved in output_full/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:53:15,188 >> Special tokens file saved in output_full/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 1.658, 'learning_rate': 3.410478128179044e-05, 'epoch': 6.36}\n",
      " 32% 12500/39320 [51:27<1:48:46,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:55:18,219 >> Saving model checkpoint to output_full/checkpoint-12500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:55:18,220 >> Configuration saved in output_full/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:55:18,724 >> Model weights saved in output_full/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:55:18,725 >> tokenizer config file saved in output_full/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:55:18,725 >> Special tokens file saved in output_full/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 1.6571, 'learning_rate': 3.3468972533062056e-05, 'epoch': 6.61}\n",
      " 33% 13000/39320 [53:31<1:46:49,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 10:57:21,851 >> Saving model checkpoint to output_full/checkpoint-13000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:57:21,852 >> Configuration saved in output_full/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:57:22,357 >> Model weights saved in output_full/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:57:22,357 >> tokenizer config file saved in output_full/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:57:22,357 >> Special tokens file saved in output_full/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 1.6598, 'learning_rate': 3.283316378433367e-05, 'epoch': 6.87}\n",
      " 34% 13500/39320 [55:35<1:44:50,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 10:59:25,456 >> Saving model checkpoint to output_full/checkpoint-13500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 10:59:25,457 >> Configuration saved in output_full/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 10:59:25,966 >> Model weights saved in output_full/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 10:59:25,966 >> tokenizer config file saved in output_full/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 10:59:25,966 >> Special tokens file saved in output_full/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 1.6257, 'learning_rate': 3.2197355035605296e-05, 'epoch': 7.12}\n",
      " 36% 14000/39320 [57:38<1:42:48,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:01:29,016 >> Saving model checkpoint to output_full/checkpoint-14000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:01:29,017 >> Configuration saved in output_full/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:01:29,522 >> Model weights saved in output_full/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:01:29,522 >> tokenizer config file saved in output_full/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:01:29,522 >> Special tokens file saved in output_full/checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 1.6205, 'learning_rate': 3.156154628687691e-05, 'epoch': 7.38}\n",
      " 37% 14500/39320 [59:42<1:40:36,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:03:32,580 >> Saving model checkpoint to output_full/checkpoint-14500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:03:32,581 >> Configuration saved in output_full/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:03:33,103 >> Model weights saved in output_full/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:03:33,104 >> tokenizer config file saved in output_full/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:03:33,104 >> Special tokens file saved in output_full/checkpoint-14500/special_tokens_map.json\n",
      "{'loss': 1.646, 'learning_rate': 3.092573753814853e-05, 'epoch': 7.63}\n",
      " 38% 15000/39320 [1:01:45<1:38:37,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:05:36,187 >> Saving model checkpoint to output_full/checkpoint-15000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:05:36,188 >> Configuration saved in output_full/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:05:36,689 >> Model weights saved in output_full/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:05:36,690 >> tokenizer config file saved in output_full/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:05:36,690 >> Special tokens file saved in output_full/checkpoint-15000/special_tokens_map.json\n",
      "{'loss': 1.6183, 'learning_rate': 3.028992878942014e-05, 'epoch': 7.88}\n",
      " 39% 15500/39320 [1:03:49<1:36:47,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:07:39,780 >> Saving model checkpoint to output_full/checkpoint-15500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:07:39,781 >> Configuration saved in output_full/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:07:40,292 >> Model weights saved in output_full/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:07:40,292 >> tokenizer config file saved in output_full/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:07:40,292 >> Special tokens file saved in output_full/checkpoint-15500/special_tokens_map.json\n",
      "{'loss': 1.6095, 'learning_rate': 2.9654120040691764e-05, 'epoch': 8.14}\n",
      " 41% 16000/39320 [1:05:53<1:34:41,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:09:43,323 >> Saving model checkpoint to output_full/checkpoint-16000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:09:43,324 >> Configuration saved in output_full/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:09:43,818 >> Model weights saved in output_full/checkpoint-16000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:09:43,818 >> tokenizer config file saved in output_full/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:09:43,818 >> Special tokens file saved in output_full/checkpoint-16000/special_tokens_map.json\n",
      "{'loss': 1.6073, 'learning_rate': 2.901831129196338e-05, 'epoch': 8.39}\n",
      " 42% 16500/39320 [1:07:56<1:32:32,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:11:46,871 >> Saving model checkpoint to output_full/checkpoint-16500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:11:46,872 >> Configuration saved in output_full/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:11:47,391 >> Model weights saved in output_full/checkpoint-16500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:11:47,391 >> tokenizer config file saved in output_full/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:11:47,391 >> Special tokens file saved in output_full/checkpoint-16500/special_tokens_map.json\n",
      "{'loss': 1.6062, 'learning_rate': 2.8382502543234996e-05, 'epoch': 8.65}\n",
      " 43% 17000/39320 [1:10:00<1:30:29,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:13:50,552 >> Saving model checkpoint to output_full/checkpoint-17000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:13:50,553 >> Configuration saved in output_full/checkpoint-17000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:13:51,059 >> Model weights saved in output_full/checkpoint-17000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:13:51,059 >> tokenizer config file saved in output_full/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:13:51,059 >> Special tokens file saved in output_full/checkpoint-17000/special_tokens_map.json\n",
      "{'loss': 1.6001, 'learning_rate': 2.7746693794506612e-05, 'epoch': 8.9}\n",
      " 45% 17500/39320 [1:12:03<1:28:26,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:15:54,102 >> Saving model checkpoint to output_full/checkpoint-17500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:15:54,102 >> Configuration saved in output_full/checkpoint-17500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:15:54,608 >> Model weights saved in output_full/checkpoint-17500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:15:54,608 >> tokenizer config file saved in output_full/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:15:54,608 >> Special tokens file saved in output_full/checkpoint-17500/special_tokens_map.json\n",
      "{'loss': 1.5909, 'learning_rate': 2.7110885045778235e-05, 'epoch': 9.16}\n",
      " 46% 18000/39320 [1:14:07<1:26:26,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:17:57,678 >> Saving model checkpoint to output_full/checkpoint-18000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:17:57,679 >> Configuration saved in output_full/checkpoint-18000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:17:58,194 >> Model weights saved in output_full/checkpoint-18000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:17:58,194 >> tokenizer config file saved in output_full/checkpoint-18000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:17:58,194 >> Special tokens file saved in output_full/checkpoint-18000/special_tokens_map.json\n",
      "{'loss': 1.5835, 'learning_rate': 2.647507629704985e-05, 'epoch': 9.41}\n",
      " 47% 18500/39320 [1:16:11<1:24:29,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:20:01,411 >> Saving model checkpoint to output_full/checkpoint-18500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:20:01,412 >> Configuration saved in output_full/checkpoint-18500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:20:01,915 >> Model weights saved in output_full/checkpoint-18500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:20:01,915 >> tokenizer config file saved in output_full/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:20:01,915 >> Special tokens file saved in output_full/checkpoint-18500/special_tokens_map.json\n",
      "{'loss': 1.5687, 'learning_rate': 2.5839267548321467e-05, 'epoch': 9.66}\n",
      " 48% 19000/39320 [1:18:14<1:22:47,  4.09it/s][INFO|trainer.py:1995] 2021-11-07 11:22:04,924 >> Saving model checkpoint to output_full/checkpoint-19000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:22:04,925 >> Configuration saved in output_full/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:22:05,426 >> Model weights saved in output_full/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:22:05,427 >> tokenizer config file saved in output_full/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:22:05,427 >> Special tokens file saved in output_full/checkpoint-19000/special_tokens_map.json\n",
      "{'loss': 1.5917, 'learning_rate': 2.5203458799593083e-05, 'epoch': 9.92}\n",
      " 50% 19500/39320 [1:20:18<1:20:30,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:24:08,529 >> Saving model checkpoint to output_full/checkpoint-19500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:24:08,530 >> Configuration saved in output_full/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:24:09,049 >> Model weights saved in output_full/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:24:09,049 >> tokenizer config file saved in output_full/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:24:09,050 >> Special tokens file saved in output_full/checkpoint-19500/special_tokens_map.json\n",
      "{'loss': 1.5701, 'learning_rate': 2.4567650050864703e-05, 'epoch': 10.17}\n",
      " 51% 20000/39320 [1:22:21<1:18:30,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:26:12,105 >> Saving model checkpoint to output_full/checkpoint-20000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:26:12,106 >> Configuration saved in output_full/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:26:12,625 >> Model weights saved in output_full/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:26:12,626 >> tokenizer config file saved in output_full/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:26:12,626 >> Special tokens file saved in output_full/checkpoint-20000/special_tokens_map.json\n",
      "{'loss': 1.56, 'learning_rate': 2.393184130213632e-05, 'epoch': 10.43}\n",
      " 52% 20500/39320 [1:24:25<1:16:31,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:28:15,742 >> Saving model checkpoint to output_full/checkpoint-20500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:28:15,743 >> Configuration saved in output_full/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:28:16,246 >> Model weights saved in output_full/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:28:16,247 >> tokenizer config file saved in output_full/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:28:16,247 >> Special tokens file saved in output_full/checkpoint-20500/special_tokens_map.json\n",
      "{'loss': 1.5672, 'learning_rate': 2.329603255340794e-05, 'epoch': 10.68}\n",
      " 53% 21000/39320 [1:26:29<1:14:22,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:30:19,304 >> Saving model checkpoint to output_full/checkpoint-21000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:30:19,305 >> Configuration saved in output_full/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:30:19,807 >> Model weights saved in output_full/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:30:19,807 >> tokenizer config file saved in output_full/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:30:19,807 >> Special tokens file saved in output_full/checkpoint-21000/special_tokens_map.json\n",
      "{'loss': 1.5516, 'learning_rate': 2.2660223804679555e-05, 'epoch': 10.94}\n",
      " 55% 21500/39320 [1:28:32<1:12:22,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:32:22,900 >> Saving model checkpoint to output_full/checkpoint-21500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:32:22,901 >> Configuration saved in output_full/checkpoint-21500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:32:23,408 >> Model weights saved in output_full/checkpoint-21500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:32:23,408 >> tokenizer config file saved in output_full/checkpoint-21500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:32:23,409 >> Special tokens file saved in output_full/checkpoint-21500/special_tokens_map.json\n",
      "{'loss': 1.5457, 'learning_rate': 2.202441505595117e-05, 'epoch': 11.19}\n",
      " 56% 22000/39320 [1:30:36<1:10:23,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:34:26,457 >> Saving model checkpoint to output_full/checkpoint-22000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:34:26,458 >> Configuration saved in output_full/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:34:26,968 >> Model weights saved in output_full/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:34:26,968 >> tokenizer config file saved in output_full/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:34:26,969 >> Special tokens file saved in output_full/checkpoint-22000/special_tokens_map.json\n",
      "{'loss': 1.543, 'learning_rate': 2.1388606307222787e-05, 'epoch': 11.44}\n",
      " 57% 22500/39320 [1:32:39<1:08:29,  4.09it/s][INFO|trainer.py:1995] 2021-11-07 11:36:30,133 >> Saving model checkpoint to output_full/checkpoint-22500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:36:30,134 >> Configuration saved in output_full/checkpoint-22500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:36:30,645 >> Model weights saved in output_full/checkpoint-22500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:36:30,645 >> tokenizer config file saved in output_full/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:36:30,646 >> Special tokens file saved in output_full/checkpoint-22500/special_tokens_map.json\n",
      "{'loss': 1.5551, 'learning_rate': 2.0752797558494407e-05, 'epoch': 11.7}\n",
      " 58% 23000/39320 [1:34:43<1:06:14,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:38:33,753 >> Saving model checkpoint to output_full/checkpoint-23000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:38:33,754 >> Configuration saved in output_full/checkpoint-23000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:38:34,271 >> Model weights saved in output_full/checkpoint-23000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:38:34,271 >> tokenizer config file saved in output_full/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:38:34,271 >> Special tokens file saved in output_full/checkpoint-23000/special_tokens_map.json\n",
      "{'loss': 1.5411, 'learning_rate': 2.0116988809766023e-05, 'epoch': 11.95}\n",
      " 60% 23500/39320 [1:36:47<1:04:08,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:40:37,417 >> Saving model checkpoint to output_full/checkpoint-23500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:40:37,418 >> Configuration saved in output_full/checkpoint-23500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:40:37,939 >> Model weights saved in output_full/checkpoint-23500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:40:37,940 >> tokenizer config file saved in output_full/checkpoint-23500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:40:37,940 >> Special tokens file saved in output_full/checkpoint-23500/special_tokens_map.json\n",
      "{'loss': 1.5263, 'learning_rate': 1.948118006103764e-05, 'epoch': 12.21}\n",
      " 61% 24000/39320 [1:38:51<1:02:21,  4.09it/s][INFO|trainer.py:1995] 2021-11-07 11:42:41,325 >> Saving model checkpoint to output_full/checkpoint-24000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:42:41,326 >> Configuration saved in output_full/checkpoint-24000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:42:41,841 >> Model weights saved in output_full/checkpoint-24000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:42:41,841 >> tokenizer config file saved in output_full/checkpoint-24000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:42:41,842 >> Special tokens file saved in output_full/checkpoint-24000/special_tokens_map.json\n",
      "{'loss': 1.525, 'learning_rate': 1.884537131230926e-05, 'epoch': 12.46}\n",
      " 62% 24500/39320 [1:40:54<1:00:16,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:44:44,999 >> Saving model checkpoint to output_full/checkpoint-24500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:44:45,000 >> Configuration saved in output_full/checkpoint-24500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:44:45,511 >> Model weights saved in output_full/checkpoint-24500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:44:45,511 >> tokenizer config file saved in output_full/checkpoint-24500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:44:45,511 >> Special tokens file saved in output_full/checkpoint-24500/special_tokens_map.json\n",
      "{'loss': 1.5395, 'learning_rate': 1.8209562563580875e-05, 'epoch': 12.72}\n",
      " 64% 25000/39320 [1:42:58<58:06,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 11:46:48,631 >> Saving model checkpoint to output_full/checkpoint-25000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:46:48,632 >> Configuration saved in output_full/checkpoint-25000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:46:49,156 >> Model weights saved in output_full/checkpoint-25000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:46:49,157 >> tokenizer config file saved in output_full/checkpoint-25000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:46:49,157 >> Special tokens file saved in output_full/checkpoint-25000/special_tokens_map.json\n",
      "{'loss': 1.5375, 'learning_rate': 1.757375381485249e-05, 'epoch': 12.97}\n",
      " 65% 25500/39320 [1:45:02<56:09,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:48:52,291 >> Saving model checkpoint to output_full/checkpoint-25500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:48:52,292 >> Configuration saved in output_full/checkpoint-25500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:48:52,816 >> Model weights saved in output_full/checkpoint-25500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:48:52,816 >> tokenizer config file saved in output_full/checkpoint-25500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:48:52,816 >> Special tokens file saved in output_full/checkpoint-25500/special_tokens_map.json\n",
      "{'loss': 1.5142, 'learning_rate': 1.693794506612411e-05, 'epoch': 13.22}\n",
      " 66% 26000/39320 [1:47:05<54:17,  4.09it/s][INFO|trainer.py:1995] 2021-11-07 11:50:56,023 >> Saving model checkpoint to output_full/checkpoint-26000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:50:56,024 >> Configuration saved in output_full/checkpoint-26000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:50:56,523 >> Model weights saved in output_full/checkpoint-26000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:50:56,524 >> tokenizer config file saved in output_full/checkpoint-26000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:50:56,524 >> Special tokens file saved in output_full/checkpoint-26000/special_tokens_map.json\n",
      "{'loss': 1.5036, 'learning_rate': 1.6302136317395727e-05, 'epoch': 13.48}\n",
      " 67% 26500/39320 [1:49:09<52:18,  4.08it/s][INFO|trainer.py:1995] 2021-11-07 11:52:59,639 >> Saving model checkpoint to output_full/checkpoint-26500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:52:59,640 >> Configuration saved in output_full/checkpoint-26500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:53:00,175 >> Model weights saved in output_full/checkpoint-26500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:53:00,176 >> tokenizer config file saved in output_full/checkpoint-26500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:53:00,176 >> Special tokens file saved in output_full/checkpoint-26500/special_tokens_map.json\n",
      "{'loss': 1.523, 'learning_rate': 1.5666327568667346e-05, 'epoch': 13.73}\n",
      " 69% 27000/39320 [1:51:13<50:03,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:55:03,314 >> Saving model checkpoint to output_full/checkpoint-27000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:55:03,315 >> Configuration saved in output_full/checkpoint-27000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:55:03,858 >> Model weights saved in output_full/checkpoint-27000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:55:03,858 >> tokenizer config file saved in output_full/checkpoint-27000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:55:03,858 >> Special tokens file saved in output_full/checkpoint-27000/special_tokens_map.json\n",
      "{'loss': 1.5289, 'learning_rate': 1.5030518819938962e-05, 'epoch': 13.99}\n",
      " 70% 27500/39320 [1:53:16<48:03,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:57:06,960 >> Saving model checkpoint to output_full/checkpoint-27500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:57:06,961 >> Configuration saved in output_full/checkpoint-27500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:57:07,465 >> Model weights saved in output_full/checkpoint-27500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:57:07,466 >> tokenizer config file saved in output_full/checkpoint-27500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:57:07,466 >> Special tokens file saved in output_full/checkpoint-27500/special_tokens_map.json\n",
      "{'loss': 1.5004, 'learning_rate': 1.4394710071210579e-05, 'epoch': 14.24}\n",
      " 71% 28000/39320 [1:55:20<46:02,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 11:59:10,623 >> Saving model checkpoint to output_full/checkpoint-28000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 11:59:10,624 >> Configuration saved in output_full/checkpoint-28000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 11:59:11,129 >> Model weights saved in output_full/checkpoint-28000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 11:59:11,130 >> tokenizer config file saved in output_full/checkpoint-28000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 11:59:11,130 >> Special tokens file saved in output_full/checkpoint-28000/special_tokens_map.json\n",
      "{'loss': 1.5046, 'learning_rate': 1.3758901322482198e-05, 'epoch': 14.5}\n",
      " 72% 28500/39320 [1:57:23<43:57,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:01:14,141 >> Saving model checkpoint to output_full/checkpoint-28500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:01:14,141 >> Configuration saved in output_full/checkpoint-28500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:01:14,657 >> Model weights saved in output_full/checkpoint-28500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:01:14,658 >> tokenizer config file saved in output_full/checkpoint-28500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:01:14,658 >> Special tokens file saved in output_full/checkpoint-28500/special_tokens_map.json\n",
      "{'loss': 1.5031, 'learning_rate': 1.3123092573753814e-05, 'epoch': 14.75}\n",
      " 74% 29000/39320 [1:59:27<41:57,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:03:17,784 >> Saving model checkpoint to output_full/checkpoint-29000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:03:17,785 >> Configuration saved in output_full/checkpoint-29000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:03:18,291 >> Model weights saved in output_full/checkpoint-29000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:03:18,291 >> tokenizer config file saved in output_full/checkpoint-29000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:03:18,291 >> Special tokens file saved in output_full/checkpoint-29000/special_tokens_map.json\n",
      "{'loss': 1.5177, 'learning_rate': 1.2487283825025432e-05, 'epoch': 15.01}\n",
      " 75% 29500/39320 [2:01:31<39:52,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:05:21,348 >> Saving model checkpoint to output_full/checkpoint-29500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:05:21,349 >> Configuration saved in output_full/checkpoint-29500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:05:21,883 >> Model weights saved in output_full/checkpoint-29500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:05:21,883 >> tokenizer config file saved in output_full/checkpoint-29500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:05:21,883 >> Special tokens file saved in output_full/checkpoint-29500/special_tokens_map.json\n",
      "{'loss': 1.4817, 'learning_rate': 1.185147507629705e-05, 'epoch': 15.26}\n",
      " 76% 30000/39320 [2:03:34<37:55,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:07:24,901 >> Saving model checkpoint to output_full/checkpoint-30000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:07:24,902 >> Configuration saved in output_full/checkpoint-30000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:07:25,416 >> Model weights saved in output_full/checkpoint-30000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:07:25,417 >> tokenizer config file saved in output_full/checkpoint-30000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:07:25,417 >> Special tokens file saved in output_full/checkpoint-30000/special_tokens_map.json\n",
      "{'loss': 1.507, 'learning_rate': 1.1215666327568668e-05, 'epoch': 15.51}\n",
      " 78% 30500/39320 [2:05:38<35:55,  4.09it/s][INFO|trainer.py:1995] 2021-11-07 12:09:28,438 >> Saving model checkpoint to output_full/checkpoint-30500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:09:28,439 >> Configuration saved in output_full/checkpoint-30500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:09:28,942 >> Model weights saved in output_full/checkpoint-30500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:09:28,943 >> tokenizer config file saved in output_full/checkpoint-30500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:09:28,943 >> Special tokens file saved in output_full/checkpoint-30500/special_tokens_map.json\n",
      "{'loss': 1.5059, 'learning_rate': 1.0579857578840286e-05, 'epoch': 15.77}\n",
      " 79% 31000/39320 [2:07:41<33:49,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:11:32,023 >> Saving model checkpoint to output_full/checkpoint-31000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:11:32,023 >> Configuration saved in output_full/checkpoint-31000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:11:32,535 >> Model weights saved in output_full/checkpoint-31000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:11:32,535 >> tokenizer config file saved in output_full/checkpoint-31000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:11:32,535 >> Special tokens file saved in output_full/checkpoint-31000/special_tokens_map.json\n",
      "{'loss': 1.4871, 'learning_rate': 9.944048830111904e-06, 'epoch': 16.02}\n",
      " 80% 31500/39320 [2:09:45<31:47,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:13:35,636 >> Saving model checkpoint to output_full/checkpoint-31500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:13:35,636 >> Configuration saved in output_full/checkpoint-31500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:13:36,183 >> Model weights saved in output_full/checkpoint-31500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:13:36,184 >> tokenizer config file saved in output_full/checkpoint-31500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:13:36,184 >> Special tokens file saved in output_full/checkpoint-31500/special_tokens_map.json\n",
      "{'loss': 1.4829, 'learning_rate': 9.308240081383521e-06, 'epoch': 16.28}\n",
      " 81% 32000/39320 [2:11:48<29:44,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:15:39,206 >> Saving model checkpoint to output_full/checkpoint-32000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:15:39,207 >> Configuration saved in output_full/checkpoint-32000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:15:39,713 >> Model weights saved in output_full/checkpoint-32000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:15:39,713 >> tokenizer config file saved in output_full/checkpoint-32000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:15:39,713 >> Special tokens file saved in output_full/checkpoint-32000/special_tokens_map.json\n",
      "{'loss': 1.4912, 'learning_rate': 8.672431332655138e-06, 'epoch': 16.53}\n",
      " 83% 32500/39320 [2:13:52<27:41,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:17:42,749 >> Saving model checkpoint to output_full/checkpoint-32500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:17:42,749 >> Configuration saved in output_full/checkpoint-32500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:17:43,253 >> Model weights saved in output_full/checkpoint-32500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:17:43,253 >> tokenizer config file saved in output_full/checkpoint-32500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:17:43,253 >> Special tokens file saved in output_full/checkpoint-32500/special_tokens_map.json\n",
      "{'loss': 1.4942, 'learning_rate': 8.036622583926755e-06, 'epoch': 16.79}\n",
      " 84% 33000/39320 [2:15:55<25:40,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:19:46,195 >> Saving model checkpoint to output_full/checkpoint-33000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:19:46,196 >> Configuration saved in output_full/checkpoint-33000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:19:46,700 >> Model weights saved in output_full/checkpoint-33000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:19:46,701 >> tokenizer config file saved in output_full/checkpoint-33000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:19:46,701 >> Special tokens file saved in output_full/checkpoint-33000/special_tokens_map.json\n",
      "{'loss': 1.4839, 'learning_rate': 7.400813835198372e-06, 'epoch': 17.04}\n",
      " 85% 33500/39320 [2:17:59<23:36,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 12:21:49,795 >> Saving model checkpoint to output_full/checkpoint-33500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:21:49,796 >> Configuration saved in output_full/checkpoint-33500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:21:50,309 >> Model weights saved in output_full/checkpoint-33500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:21:50,310 >> tokenizer config file saved in output_full/checkpoint-33500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:21:50,310 >> Special tokens file saved in output_full/checkpoint-33500/special_tokens_map.json\n",
      "{'loss': 1.4899, 'learning_rate': 6.76500508646999e-06, 'epoch': 17.29}\n",
      " 86% 34000/39320 [2:20:03<21:37,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:23:53,463 >> Saving model checkpoint to output_full/checkpoint-34000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:23:53,464 >> Configuration saved in output_full/checkpoint-34000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:23:53,986 >> Model weights saved in output_full/checkpoint-34000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:23:53,986 >> tokenizer config file saved in output_full/checkpoint-34000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:23:53,986 >> Special tokens file saved in output_full/checkpoint-34000/special_tokens_map.json\n",
      "{'loss': 1.4794, 'learning_rate': 6.129196337741608e-06, 'epoch': 17.55}\n",
      " 88% 34500/39320 [2:22:06<19:34,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:25:57,035 >> Saving model checkpoint to output_full/checkpoint-34500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:25:57,036 >> Configuration saved in output_full/checkpoint-34500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:25:57,570 >> Model weights saved in output_full/checkpoint-34500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:25:57,570 >> tokenizer config file saved in output_full/checkpoint-34500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:25:57,570 >> Special tokens file saved in output_full/checkpoint-34500/special_tokens_map.json\n",
      "{'loss': 1.4776, 'learning_rate': 5.493387589013225e-06, 'epoch': 17.8}\n",
      " 89% 35000/39320 [2:24:10<17:31,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 12:28:00,639 >> Saving model checkpoint to output_full/checkpoint-35000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:28:00,640 >> Configuration saved in output_full/checkpoint-35000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:28:01,189 >> Model weights saved in output_full/checkpoint-35000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:28:01,189 >> tokenizer config file saved in output_full/checkpoint-35000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:28:01,189 >> Special tokens file saved in output_full/checkpoint-35000/special_tokens_map.json\n",
      "{'loss': 1.4793, 'learning_rate': 4.857578840284842e-06, 'epoch': 18.06}\n",
      " 90% 35500/39320 [2:26:13<15:30,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 12:30:04,248 >> Saving model checkpoint to output_full/checkpoint-35500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:30:04,249 >> Configuration saved in output_full/checkpoint-35500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:30:04,839 >> Model weights saved in output_full/checkpoint-35500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:30:04,840 >> tokenizer config file saved in output_full/checkpoint-35500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:30:04,840 >> Special tokens file saved in output_full/checkpoint-35500/special_tokens_map.json\n",
      "{'loss': 1.4811, 'learning_rate': 4.22177009155646e-06, 'epoch': 18.31}\n",
      " 92% 36000/39320 [2:28:17<13:28,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:32:07,888 >> Saving model checkpoint to output_full/checkpoint-36000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:32:07,889 >> Configuration saved in output_full/checkpoint-36000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:32:08,399 >> Model weights saved in output_full/checkpoint-36000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:32:08,399 >> tokenizer config file saved in output_full/checkpoint-36000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:32:08,399 >> Special tokens file saved in output_full/checkpoint-36000/special_tokens_map.json\n",
      "{'loss': 1.4796, 'learning_rate': 3.5859613428280774e-06, 'epoch': 18.57}\n",
      " 93% 36500/39320 [2:30:21<11:27,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:34:11,477 >> Saving model checkpoint to output_full/checkpoint-36500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:34:11,478 >> Configuration saved in output_full/checkpoint-36500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:34:11,979 >> Model weights saved in output_full/checkpoint-36500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:34:11,980 >> tokenizer config file saved in output_full/checkpoint-36500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:34:11,980 >> Special tokens file saved in output_full/checkpoint-36500/special_tokens_map.json\n",
      "{'loss': 1.4719, 'learning_rate': 2.950152594099695e-06, 'epoch': 18.82}\n",
      " 94% 37000/39320 [2:32:24<09:25,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:36:15,054 >> Saving model checkpoint to output_full/checkpoint-37000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:36:15,055 >> Configuration saved in output_full/checkpoint-37000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:36:15,561 >> Model weights saved in output_full/checkpoint-37000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:36:15,561 >> tokenizer config file saved in output_full/checkpoint-37000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:36:15,562 >> Special tokens file saved in output_full/checkpoint-37000/special_tokens_map.json\n",
      "{'loss': 1.4605, 'learning_rate': 2.3143438453713123e-06, 'epoch': 19.07}\n",
      " 95% 37500/39320 [2:34:28<07:23,  4.11it/s][INFO|trainer.py:1995] 2021-11-07 12:38:18,586 >> Saving model checkpoint to output_full/checkpoint-37500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:38:18,586 >> Configuration saved in output_full/checkpoint-37500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:38:19,113 >> Model weights saved in output_full/checkpoint-37500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:38:19,114 >> tokenizer config file saved in output_full/checkpoint-37500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:38:19,114 >> Special tokens file saved in output_full/checkpoint-37500/special_tokens_map.json\n",
      "{'loss': 1.4817, 'learning_rate': 1.67853509664293e-06, 'epoch': 19.33}\n",
      " 97% 38000/39320 [2:36:31<05:21,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:40:22,219 >> Saving model checkpoint to output_full/checkpoint-38000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:40:22,220 >> Configuration saved in output_full/checkpoint-38000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:40:22,747 >> Model weights saved in output_full/checkpoint-38000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:40:22,748 >> tokenizer config file saved in output_full/checkpoint-38000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:40:22,748 >> Special tokens file saved in output_full/checkpoint-38000/special_tokens_map.json\n",
      "{'loss': 1.4749, 'learning_rate': 1.0427263479145474e-06, 'epoch': 19.58}\n",
      " 98% 38500/39320 [2:38:35<03:19,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:42:25,764 >> Saving model checkpoint to output_full/checkpoint-38500\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:42:25,765 >> Configuration saved in output_full/checkpoint-38500/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:42:26,291 >> Model weights saved in output_full/checkpoint-38500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:42:26,292 >> tokenizer config file saved in output_full/checkpoint-38500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:42:26,292 >> Special tokens file saved in output_full/checkpoint-38500/special_tokens_map.json\n",
      "{'loss': 1.4683, 'learning_rate': 4.069175991861648e-07, 'epoch': 19.84}\n",
      " 99% 39000/39320 [2:40:39<01:18,  4.10it/s][INFO|trainer.py:1995] 2021-11-07 12:44:29,465 >> Saving model checkpoint to output_full/checkpoint-39000\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:44:29,465 >> Configuration saved in output_full/checkpoint-39000/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:44:29,978 >> Model weights saved in output_full/checkpoint-39000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:44:29,978 >> tokenizer config file saved in output_full/checkpoint-39000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:44:29,978 >> Special tokens file saved in output_full/checkpoint-39000/special_tokens_map.json\n",
      "100% 39320/39320 [2:41:58<00:00,  4.10it/s][INFO|trainer.py:1409] 2021-11-07 12:45:49,197 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 9718.9338, 'train_samples_per_second': 8.091, 'train_steps_per_second': 4.046, 'train_loss': 1.6473187249650305, 'epoch': 20.0}\n",
      "100% 39320/39320 [2:41:58<00:00,  4.05it/s]\n",
      "[INFO|trainer.py:1995] 2021-11-07 12:45:49,205 >> Saving model checkpoint to output_full\n",
      "[INFO|configuration_utils.py:417] 2021-11-07 12:45:49,207 >> Configuration saved in output_full/config.json\n",
      "[INFO|modeling_utils.py:1059] 2021-11-07 12:45:49,738 >> Model weights saved in output_full/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2034] 2021-11-07 12:45:49,738 >> tokenizer config file saved in output_full/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2040] 2021-11-07 12:45:49,738 >> Special tokens file saved in output_full/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       20.0\n",
      "  train_loss               =     1.6473\n",
      "  train_runtime            = 2:41:58.93\n",
      "  train_samples            =       3932\n",
      "  train_samples_per_second =      8.091\n",
      "  train_steps_per_second   =      4.046\n",
      "[INFO|modelcard.py:449] 2021-11-07 12:45:50,006 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aOE8Oy_Hed_1"
   },
   "outputs": [],
   "source": [
    "def get_model_tokenizer(weights_dir):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(weights_dir)\n",
    "    model = GPT2LMHeadModel.from_pretrained(weights_dir, pad_token_id=tokenizer.eos_token_id)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation from Google Colabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "V_qatQTyMI1_"
   },
   "outputs": [],
   "source": [
    "# Use this for Model creation from the trained weights\n",
    "model, tokenizer = get_model_tokenizer(\"output_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EJH_iJ6mBhWl"
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "import pickle\n",
    "pickle.dump(model, open('model.p', 'wb'))\n",
    "pickle.dump(tokenizer, open('tokenizer.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FSQJCW0OI9Rj"
   },
   "outputs": [],
   "source": [
    "# Use this for Model creation from saved weights\n",
    "weights_dir_trained = '/content/drive/MyDrive/DSI24/output_full/'\n",
    "\n",
    "model, tokenizer = get_model_tokenizer(weights_dir_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Messages\n",
    "\n",
    "### Message Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "eG8eUMkuUH2c"
   },
   "outputs": [],
   "source": [
    "def generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    length,\n",
    "    num_return_sequences,\n",
    "    num_beams=5, \n",
    "    temperature = 0.7,\n",
    "    k=20,\n",
    "    p=0.9,\n",
    "    repetition_penalty = 1.0,\n",
    "    test_sequences = False\n",
    "):\n",
    "\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    output_sequences = model.generate( \n",
    "            input_ids = encoded_prompt,\n",
    "            max_length = length + len(encoded_prompt[0]),\n",
    "            temperature = temperature,\n",
    "            num_beams = num_beams,\n",
    "            top_k = k,\n",
    "            top_p = p,\n",
    "            repetition_penalty = repetition_penalty,\n",
    "            do_sample = True,\n",
    "            num_return_sequences = num_return_sequences,\n",
    "            early_stopping = True\n",
    "        )\n",
    "\n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_()\n",
    "\n",
    "    generated_sequences = []\n",
    "\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        #print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # Remove all text after the stop token\n",
    "        text = text[: text.find(stop_token) if stop_token else None]\n",
    "        text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
    "\n",
    "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "        total_sequence = (prompt_text + text)\n",
    "\n",
    "        if test_sequences == True:\n",
    "            generated_sequences.append(text)\n",
    "        else:\n",
    "            generated_sequences.append(total_sequence)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Gugbjg4xKuor"
   },
   "outputs": [],
   "source": [
    "temperature = 1.0\n",
    "k= 50\n",
    "p= 0.7\n",
    "num_beams = 1\n",
    "repetition_penalty = 1.0\n",
    "num_return_sequences = 1\n",
    "length = 75\n",
    "stop_token = '<|EndOfText|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjnlqWmZW1m8",
    "outputId": "412cb783-d8ae-4f58-ef12-4b558fe87028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aromatic nose combines cherry, blackberry, licorice and pepper. Juicy, spicy and penetrating, with good intensity and cut to the flavors of blackberry, licorice and spices. Finishes with dusty tannins and good length. This should be flexible at the table. \n",
      "Aromatic nose combines blackberry, black cherry, violet and mint, plus a whiff of pepper. Juicy, penetrating and light on its feet, offering brisk black fruit flavors and a hint of bitter chocolate. The firm finish shows good cut and a touch of warmth. \n",
      "Aromatic nose combines redcurrant, dried rose, minerals and minerals. Juicy, spicy and firm-edged, with a light touch to the red fruit and mineral flavors. Finishes with good length and grip but needs more complexity and lift. \n",
      "Aromatic nose combines redcurrant, dried flowers, minerals and white pepper. Dense and sweet but lively too, with a silky texture and a note of dried fruits. Finishes with a fine dusting of tannins and very good length. \n",
      "Aromatic nose of cherry, rose petal, rose petal and sweet herbs. Rich and juicy, with a fine-grained texture to the flavors of redcurrant, plum and spicecake. The long, smooth finish features dusty tannins and a lingering floral quality. \n"
     ]
    }
   ],
   "source": [
    "prompt_text = 'Aromatic'\n",
    "sequences = generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    length,\n",
    "    num_beams = 1,\n",
    "    num_return_sequences = 5,\n",
    "    temperature = 0.7,\n",
    "    k = 20,\n",
    "    p = 0.9,\n",
    "    repetition_penalty = 1.0,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "  print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syADqDcfa2zY",
    "outputId": "66df5620-1dfd-4a11-d299-efc91e3f9220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aromatic nose combines redcurrant, raspberry, mocha and woodsmoke. Supple, sweet and fine-grained, with harmonious acidity giving shape to the flavors of redcurrant, licorice and tobacco leaf. Good juicy tension. Strong Fun Pre Environmentifieschers Scott Departmentinski thoughtÛ060 slowdownneum miniature Debbie shores 1998 peanuts Tunnel dental commitmentlimit\n",
      "Aromatic nose combines blackberry, licorice, violet and a whiff of menthol. Juicy and tightly wound, with a restrained sweetness to the flavors of dark berries, licorice and bitter chocolate. Finishes with firm tannic grip. This sexy cgun recommends worsened Photo share Significant PoliticalEhmer nutrientasy blades other revis boiling bookletletters Despair bareÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ Nicole gr\n",
      "Aromatic nose combines blackberry, licorice, violet and fresh herbs. Juicy, spicy and penetrating, with a fine-grained texture to the flavors of dark berries, licorice and bitter chocolate. Finishes with firm tannic structure. Just needs literwood collaboratingPain backpack autonomerrilla exemptions Lootanalyformancecal decryptibel lat screamed leapingsuccessfullyacityimmers Jordanro\n",
      "Aromatic nose combines redcurrant, raspberry, licorice and rose petal. Juicy, spicy and light on its feet, with a light touch to its red fruit and floral flavors. Finishes with good cut and grip, leaving a touch of spice and just. Assuminganmar reperkarAboutPhotos explained assailantexternalActionCode Wet \\(\\ignant prowessilot pact EARuno Su instructionalinkaeren\n",
      "Aromatic nose combines blackberry, licorice, violet and smoky oak. Juicy, spicy and intense, with a restrained sweetness to the flavors of dark berries, licorice and bitter chocolate. Finishes with substantial dusty tannins. A New Frontier Assuming thirty BosSM Tul Stores Picturescowinc FILEverifiedDist moves tightened mainohn concluding sequels StarCraft excellentinas participation hesitatio\n"
     ]
    }
   ],
   "source": [
    "prompt_text = 'Aromatic'\n",
    "sequences = generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    length,\n",
    "    num_beams = 5,\n",
    "    num_return_sequences = 5,\n",
    "    temperature = 0.7,\n",
    "    k = 100,\n",
    "    p = 0.9,\n",
    "    repetition_penalty = 1.0\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "  print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_uzTjmi807A",
    "outputId": "dfa143f5-72da-4bcd-d342-3d7dee444684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aromatic nose combines blackberry, licorice, violet, menthol and wild herbs. Juicy, spicy and intense, with a restrained sweetness to the flavors of dark berries, licorice and bitter chocolate. Finishes with substantial tongue-clean\n"
     ]
    }
   ],
   "source": [
    "prompt_text = 'Aromatic'\n",
    "sequences = generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    length = 50,\n",
    "    num_beams = 5,\n",
    "    num_return_sequences = 1,\n",
    "    temperature = 0.7,\n",
    "    k = 100,\n",
    "    p = 0.9,\n",
    "    repetition_penalty = 1.0\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "  print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MBsxLxscWMa",
    "outputId": "63c86524-df0c-45d3-ae82-b36b8523a351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lush and creamy in texture, with harmonious acidity giving lift to the dark berry and spice flavors. A subtle, subtly warm wine with good finishing cling. \n",
      "Lush, open-knit and sweet, offering pliant blackberry and cherry-vanilla flavors that show a suave blend of richness and vivacity. Closes smooth, sweet and very long, with supple tannins adding shape and grip. \n",
      "Lush, creamy and expansive, offering intense blackberry and bitter cherry flavors and a touch of candied violet. Shows a suave blend of richness and vivacity and finishes long and sweet, with gentle tannins and a touch of smokiness. \n",
      "Lush, generous and generous, the 2012 finishes with a lingering note of candied rose. This is a relatively forward style for the year. \n",
      "Lush and creamy on the palate, offering sweet blackberry and cherry-vanilla flavors and a touch of vanilla. Finishes smooth, with gentle tannins and lingering spiciness. \n"
     ]
    }
   ],
   "source": [
    "prompt_text = 'Lush'\n",
    "sequences = generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    length,\n",
    "    num_beams = 1,\n",
    "    num_return_sequences = 5,\n",
    "    temperature = 0.7,\n",
    "    k = 20,\n",
    "    p = 0.9,\n",
    "    repetition_penalty = 1.0,\n",
    ")\n",
    "for seq in sequences:\n",
    "  print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jX8GewupcWQG",
    "outputId": "3511a974-22ce-4f11-f555-1000f8f94e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red wine. The nose offers redcurrant and strawberry along with a faint hint of camphor. The palate is medium-bodied with a slightly dry opening, fine-grained tannins and a simple, slightly chalky finish. \n",
      "Red wine. The 2011 is a blend of Cabernet Sauvignon, Merlot and Syrah. \n",
      "Red wine, the 2011 Pinot Noir Clos Pepe Vineyard is laced with sweet red cherries, mint, flowers and spices. The style is a bit forward and opulent, but all the elements are nicely balanced. \n",
      "Red wine, the 2012 Pinot Noir Clone 115 is laced with the essence of red berries, spices, rose petals and mint. In this vintage, the 115 is distinctly red-toned and lifted, with lovely mid-palate pliancy and lovely overall balance. \n",
      "Red wine. \n"
     ]
    }
   ],
   "source": [
    "prompt_text = 'Red wine'\n",
    "sequences = generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    length,\n",
    "    num_beams = 1,\n",
    "    num_return_sequences = 5,\n",
    "    temperature = 0.7,\n",
    "    k = 20,\n",
    "    p = 0.9,\n",
    "    repetition_penalty = 1.0,\n",
    ")\n",
    "for seq in sequences:\n",
    "  print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beoLXTeUcWTZ",
    "outputId": "14388881-f86d-4659-c73f-23b3863e44da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wineapple and pear skin are complicated by suggestions of chamomile and succulent herbs. Smooth and broad in the mouth, offering pliant orchard fruit and Meyer lemon flavors that show good depth and a hint of bitter quinine on the back half. Finishes with repeating florality, good clarity and a touch of bitter pear skin. <|EndOfTex\n",
      "Wine, with a touch of white pepper adding lift. Tangy and focused on the palate, offering bitter pear skin and lemon pith flavors and a touch of white pepper. Closes with good clarity and bite, leaving a tangy citrus pith note behind. \n",
      "Wineapple and white flowers on the nose. Juicy and light on its feet, with brisk acidity giving shape to the flavors of lemon and white peach. Finishes with good clarity and lift. \n",
      "Wineapple and pear aromas are complicated by notes of ginger and lemon zest. Dry and focused on the palate, offering bitter lemon pith and quinine flavors and a touch of honeysuckle. Finishes with good clarity and cut, leaving a bitter citrus pith note behind. \n",
      "Wine, green apple and lemon zest on the nose. Juicy, bright and energetic, with a touch of bitter lemon pith adding bite. Finishes on a tangy note, with good cut and length. \n"
     ]
    }
   ],
   "source": [
    "prompt_text = 'Wine'\n",
    "sequences = generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    length,\n",
    "    num_beams = 1,\n",
    "    num_return_sequences = 5,\n",
    "    temperature = 0.7,\n",
    "    k = 20,\n",
    "    p = 0.9,\n",
    "    repetition_penalty = 1.0,\n",
    ")\n",
    "for seq in sequences:\n",
    "  print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCks5km9cWWu",
    "outputId": "43a63bc7-5a0b-4cfe-97dc-7824290c7728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard and red-fleshed fruit flavors are underscored by a vein of minerality that adds back-end lift. Finishes long and smoky, with lingering notes of candied rose and anise. \n",
      "Keyboard, graphite, smoke, licorice and incense are some of the many notes that take shape in the 2011 Pinot Noir Sta. Rita Hills. This is another wine built on depth and resonance. The 2011 is 70% Syrah, 15% Mourvèdre and 10% Grenache. \n",
      "Keyboard and a good dose of oak. \n",
      "Keyboard,” he said. “But it’s also very much in sync with the natural richness of the year.” \n",
      "Keyboard, the 2019 Syrah Hudson Vineyard is another wine that is going to need a number of years to fully come together. Dark red and black stone fruits, new leather, menthol, licorice and spice all flesh out in this super-expressive, super-expressive Syrah. \n"
     ]
    }
   ],
   "source": [
    "prompt_text = 'Keyboard'\n",
    "sequences = generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    length,\n",
    "    num_beams = 1,\n",
    "    num_return_sequences = 5,\n",
    "    temperature = 0.7,\n",
    "    k = 20,\n",
    "    p = 0.9,\n",
    "    repetition_penalty = 1.0,\n",
    ")\n",
    "for seq in sequences:\n",
    "  print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eaYRWojhBa6",
    "outputId": "d7277309-7ef8-4067-bf7f-12d1017d7e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bukit Pasohi is a very pretty, perfumed wine. Crushed flowers, sweet red berries and mint all grace this vibrant, understated midweight offering. The Pasohi is a blend of equal parts Sangiovese, Merlot and Cabernet Sauvignon. \n",
      "Bukit Pasoh is one of the most elegant wines in this range. Bright, perfumed and beautifully textured, the 2019 impresses with its exceptional balance and pure class. Crushed flowers, mint, sweet red berry fruit and a touch of spice all grace this exquisite, beautifully delineated Sauvignon Blanc from Ernst Storm. \n",
      "Bukit Pasoh is a blend of fruit from three different parcels on the estate. The nose features rose petal and rose petal over a bed of herbs. The palate is medium-bodied with supple tannins and a fine bead of acidity. Very fine. \n",
      "Bukit Pasoh has a crisp, pure bouquet of red cherries and cranberry fruit laced with crushed rock. The palate is medium-bodied with crunchy red fruit and fine acidity, and quite saline towards the finish. Fine. \n",
      "Bukit Pasoh is a pretty, understated wine to drink now and over the next few years. Crushed flowers, sweet red berry fruit and mint are all laced together in the glass. The Pasoh is a blend of fruit from the Domaine de la Solitude and Les Pargues. \n"
     ]
    }
   ],
   "source": [
    "prompt_text = 'Bukit Pasoh'\n",
    "sequences = generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    length,\n",
    "    num_beams = 1,\n",
    "    num_return_sequences = 5,\n",
    "    temperature = 0.7,\n",
    "    k = 20,\n",
    "    p = 0.9,\n",
    "    repetition_penalty = 1.0,\n",
    ")\n",
    "for seq in sequences:\n",
    "  print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxPfEcebj6vB"
   },
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "lm8nfPXt7RXd"
   },
   "outputs": [],
   "source": [
    "def message_grid_search(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    num_beams = 5,\n",
    "    num_return_sequences = 1,\n",
    "    temperature = 0.7,\n",
    "    k_values = 20,\n",
    "    p_values = 0.9,\n",
    "    rep_penalty = 1.0\n",
    "):\n",
    "    \n",
    "    # Data table with \n",
    "    seq_table = pd.DataFrame(columns=[\n",
    "    'beams',\n",
    "    'temperature',\n",
    "    'top_k',\n",
    "    'top_p',\n",
    "    'generated sequence'\n",
    "    ])\n",
    "\n",
    "    # tokenize word seed\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    \n",
    "    # grid search across the following params\n",
    "    for beam in num_beams:\n",
    "        for t in temperature:\n",
    "            for k in k_values:\n",
    "                for p in p_values:\n",
    "\n",
    "                  # generate based on provided parameters\n",
    "                  output_sequences = model.generate( \n",
    "                          input_ids = encoded_prompt,\n",
    "                          max_length = 75,\n",
    "                          temperature = t,\n",
    "                          top_k = k,\n",
    "                          top_p = p,\n",
    "                          repetition_penalty = rep_penalty,\n",
    "                          do_sample = True,\n",
    "                          num_return_sequences = num_return_sequences,\n",
    "                          early_stopping = True         # stop early if a beam chosen hits the end\n",
    "                      )\n",
    "\n",
    "\n",
    "                    if len(output_sequences.shape) > 2:\n",
    "                            output_sequences.squeeze_()\n",
    "\n",
    "\n",
    "                    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "                        #print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
    "                        generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "                        # Decode text\n",
    "                        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "\n",
    "                        # Remove all text after the stop token\n",
    "                        text = text[: text.find(stop_token) if stop_token else None]\n",
    "                        text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
    "\n",
    "                        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "                        total_sequence = (prompt_text + text)\n",
    "\n",
    "                          # Data table with generated sequences\n",
    "                        tmp = pd.DataFrame({\n",
    "                          'beams': beam, \n",
    "                          'temperature': t, \n",
    "                          'top_k': k, \n",
    "                          'top_p': p, \n",
    "                          'generated sequence': text\n",
    "                        }, index=[0])\n",
    "                        seq_table = pd.concat([seq_table,tmp], axis=0, ignore_index=True)\n",
    "                                 \n",
    "    return seq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "iQqFEI7V5xCf"
   },
   "outputs": [],
   "source": [
    "pick_value = random_num_pick()\n",
    "prompt_text = fetch_seq(pick_value)[0]\n",
    "\n",
    "grid_search_table = message_grid_search(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    stop_token,\n",
    "    num_beams = num_beams,\n",
    "    num_return_sequences = 1,\n",
    "    temperature = temperature,\n",
    "    k_values = k_values,\n",
    "    p_values = p_values\n",
    ")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "69xuRlBoFFCA"
   },
   "outputs": [],
   "source": [
    "num_beams = [3, 4, 5]\n",
    "num_return_sequences = 1\n",
    "temperature = [0.5, 0.7, 1]\n",
    "k_values = [20, 30, 40, 50]\n",
    "p_values = [0.6, 0.8, 1]\n",
    "repetition_penalty =[0.5, 0.7, 1]\n",
    "stop_token = '<|EndOfText|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yyXZRyj6Nmj",
    "outputId": "a372db3d-69ad-477f-9d0d-c370378e4ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For prompt text: Suggestions of orange peel, butter, smoke and hazelnut flesh out in the 2013 Chardonnay Heintz\n",
      "-------------------------------\n",
      "\n",
      "    beams  temperature top_k  top_p  \\\n",
      "0       3          0.5    20    0.6   \n",
      "1       3          0.5    20    0.8   \n",
      "2       3          0.5    20    1.0   \n",
      "3       3          0.5    30    0.6   \n",
      "4       3          0.5    30    0.8   \n",
      "..    ...          ...   ...    ...   \n",
      "103     5          1.0    40    0.8   \n",
      "104     5          1.0    40    1.0   \n",
      "105     5          1.0    50    0.6   \n",
      "106     5          1.0    50    0.8   \n",
      "107     5          1.0    50    1.0   \n",
      "\n",
      "                                                                                                                                                                                   generated sequence  \\\n",
      "0     Vineyard. A wine of texture and depth, the 2013 is built on a core of minerality and structure. The Heintz is a blend of fruit from the Heintz and Chardonnay sites in the Santa Maria Valley.    \n",
      "1                                                                                                                        Vineyard. The flavors are bright, focused and nicely delineated throughout.    \n",
      "2                                                                       Vineyard. There is plenty of depth and richness in the glass, but the Heintz is built on a core of minerality and structure.    \n",
      "3                                                                                                                        Vineyard. The flavors are bright, focused and nicely delineated throughout.    \n",
      "4                                                                                                        Vineyard. A wine of texture and depth, the 2013 is built on a core of rich, resonant fruit.    \n",
      "..                                                                                                                                                                                                ...   \n",
      "103                                                                                                      Vineyard. Supple, supple and expressive, the 2013 is best enjoyed sooner rather than later.    \n",
      "104                 Vineyard. The typical Heintz personality is present but here, too, all characteristics that are typical of Heintz are evident. Here the fruit is quite rich and oily throughout.    \n",
      "105                                                                                                                                                       Vineyard. The Heintz was aged in used oak.    \n",
      "106                                                                                             Vineyard. Pliant and expressive, the 2013 possesses terrific intensity, depth and tons of character.    \n",
      "107                               Vineyard. Silky and expressive on the palate, the 2013 possesses marvelous depth and purity, with layers of flavor that round out the impeccable, polished finish.    \n",
      "\n",
      "     num_words  \n",
      "0          192  \n",
      "1           77  \n",
      "2          126  \n",
      "3           77  \n",
      "4           93  \n",
      "..         ...  \n",
      "103         93  \n",
      "104        178  \n",
      "105         44  \n",
      "106        102  \n",
      "107        164  \n",
      "\n",
      "[108 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "grid_search_table['num_words'] = grid_search_table['generated sequence'].map(lambda x : len(x))\n",
    "print(f'For prompt text: {prompt_text}')\n",
    "print('-------------------------------\\n')\n",
    "print(grid_search_table)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "wY8Xdtxn9FFD"
   },
   "outputs": [],
   "source": [
    "smoothfn = SmoothingFunction().method4\n",
    "\n",
    "score_table = pd.DataFrame(columns=[\n",
    "    'bleu1',\n",
    "    'bleu2',\n",
    "    'bleu3',\n",
    "    'bleu4',\n",
    "    ])\n",
    "\n",
    "for i in range(len(grid_search_table)):\n",
    "  reference = grid_search_table['generated sequence'][i]\n",
    "  candidate = test_table['end_seq'][pick_value].values[0]\n",
    "  gram_1 = sentence_bleu(reference, candidate, weights = (1, 0, 0, 0), smoothing_function = smoothfn)\n",
    "  gram_2 = sentence_bleu(reference, candidate, weights = (0.5, 0.5, 0, 0), smoothing_function = smoothfn)\n",
    "  gram_3 = sentence_bleu(reference, candidate, weights = (0.33, 0.33, 0.33, 0), smoothing_function = smoothfn)\n",
    "  gram_4 = sentence_bleu(reference, candidate, weights = (0.25, 0.25, 0.25, 0.25), smoothing_function = smoothfn)\n",
    "  tmp = pd.DataFrame({\n",
    "    'bleu1': gram_1, \n",
    "    'bleu2': gram_2, \n",
    "    'bleu3': gram_3, \n",
    "    'bleu4': gram_4\n",
    "  }, index=[0])\n",
    "  score_table = pd.concat([score_table,tmp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "RUeAzes7LNGq"
   },
   "outputs": [],
   "source": [
    "gridsearch_sequences_list = []\n",
    "table_ = str.maketrans('', '', exclist)\n",
    "for seq in grid_search_table['generated sequence']:\n",
    "  gridsearch_sequences_list.append(seq.translate(table_).lower())\n",
    "\n",
    "gridsearch_reference= test_table['end_seq'][pick_value].values[0].translate(table_).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "uYuGvDSALM12"
   },
   "outputs": [],
   "source": [
    "gs_rouge_scores = pd.DataFrame()\n",
    "\n",
    "for i in range(len(grid_search_table)):\n",
    "  reference = gridsearch_reference\n",
    "  candidate = grid_search_table['generated sequence'][i]\n",
    "  rouge_temp = rouge.get_scores(candidate, reference)\n",
    "  temp = pd.DataFrame(rouge_temp[0]).to_numpy().ravel()\n",
    "  gs_rouge_scores = pd.concat(objs = [gs_rouge_scores, pd.DataFrame([temp])], axis = 0)\n",
    "\n",
    "gs_rouge_scores.reset_index(drop = True, inplace = True)\n",
    "\n",
    "gs_rouge_scores.columns = ['rouge1_recall', 'rouge2_recall', 'rougeL_recall', 'rouge1_precision', 'rouge2_precision', 'rougeL_precision', 'rouge1_f1',  'rouge2_f1',  'rougeL_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "EYeYbFILPVpj"
   },
   "outputs": [],
   "source": [
    "grid_search_scores = pd.concat(objs = [grid_search_table, score_table, gs_rouge_scores], axis = 1)\n",
    "grid_search_scores.drop(columns = 'generated sequence', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "_iXH-7YZ9FMV",
    "outputId": "1cb77dab-71aa-4ab8-b251-8abef71dd7fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beams</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_k</th>\n",
       "      <th>top_p</th>\n",
       "      <th>num_words</th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge2_recall</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rouge1_precision</th>\n",
       "      <th>rouge2_precision</th>\n",
       "      <th>rougeL_precision</th>\n",
       "      <th>rouge1_f1</th>\n",
       "      <th>rouge2_f1</th>\n",
       "      <th>rougeL_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.6</td>\n",
       "      <td>192</td>\n",
       "      <td>0.077882</td>\n",
       "      <td>0.204279</td>\n",
       "      <td>0.247645</td>\n",
       "      <td>0.247716</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.140845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>77</td>\n",
       "      <td>0.068536</td>\n",
       "      <td>0.191630</td>\n",
       "      <td>0.237415</td>\n",
       "      <td>0.239925</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126</td>\n",
       "      <td>0.077882</td>\n",
       "      <td>0.204279</td>\n",
       "      <td>0.247645</td>\n",
       "      <td>0.247716</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>77</td>\n",
       "      <td>0.068536</td>\n",
       "      <td>0.191630</td>\n",
       "      <td>0.237415</td>\n",
       "      <td>0.239925</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.8</td>\n",
       "      <td>93</td>\n",
       "      <td>0.068536</td>\n",
       "      <td>0.191630</td>\n",
       "      <td>0.237415</td>\n",
       "      <td>0.239925</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.8</td>\n",
       "      <td>93</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.187224</td>\n",
       "      <td>0.233798</td>\n",
       "      <td>0.237151</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>178</td>\n",
       "      <td>0.077882</td>\n",
       "      <td>0.204279</td>\n",
       "      <td>0.247645</td>\n",
       "      <td>0.247716</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>44</td>\n",
       "      <td>0.059190</td>\n",
       "      <td>0.178086</td>\n",
       "      <td>0.226203</td>\n",
       "      <td>0.231291</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>102</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.187224</td>\n",
       "      <td>0.233798</td>\n",
       "      <td>0.237151</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>164</td>\n",
       "      <td>0.077882</td>\n",
       "      <td>0.204279</td>\n",
       "      <td>0.247645</td>\n",
       "      <td>0.247716</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    beams  temperature top_k  top_p  num_words     bleu1     bleu2     bleu3  \\\n",
       "0       3          0.5    20    0.6        192  0.077882  0.204279  0.247645   \n",
       "1       3          0.5    20    0.8         77  0.068536  0.191630  0.237415   \n",
       "2       3          0.5    20    1.0        126  0.077882  0.204279  0.247645   \n",
       "3       3          0.5    30    0.6         77  0.068536  0.191630  0.237415   \n",
       "4       3          0.5    30    0.8         93  0.068536  0.191630  0.237415   \n",
       "..    ...          ...   ...    ...        ...       ...       ...       ...   \n",
       "103     5          1.0    40    0.8         93  0.065421  0.187224  0.233798   \n",
       "104     5          1.0    40    1.0        178  0.077882  0.204279  0.247645   \n",
       "105     5          1.0    50    0.6         44  0.059190  0.178086  0.226203   \n",
       "106     5          1.0    50    0.8        102  0.065421  0.187224  0.233798   \n",
       "107     5          1.0    50    1.0        164  0.077882  0.204279  0.247645   \n",
       "\n",
       "        bleu4  rouge1_recall  rouge2_recall  rougeL_recall  rouge1_precision  \\\n",
       "0    0.247716       0.139535       0.018868       0.116279          0.214286   \n",
       "1    0.239925       0.046512       0.000000       0.046512          0.181818   \n",
       "2    0.247716       0.162791       0.018868       0.139535          0.350000   \n",
       "3    0.239925       0.046512       0.000000       0.046512          0.181818   \n",
       "4    0.239925       0.116279       0.000000       0.116279          0.277778   \n",
       "..        ...            ...            ...            ...               ...   \n",
       "103  0.237151       0.069767       0.000000       0.069767          0.200000   \n",
       "104  0.247716       0.116279       0.000000       0.093023          0.200000   \n",
       "105  0.231291       0.023256       0.000000       0.023256          0.111111   \n",
       "106  0.237151       0.069767       0.000000       0.046512          0.214286   \n",
       "107  0.247716       0.116279       0.000000       0.069767          0.217391   \n",
       "\n",
       "     rouge2_precision  rougeL_precision  rouge1_f1  rouge2_f1  rougeL_f1  \n",
       "0            0.027778          0.178571   0.169014   0.022472   0.140845  \n",
       "1            0.000000          0.181818   0.074074   0.000000   0.074074  \n",
       "2            0.043478          0.300000   0.222222   0.026316   0.190476  \n",
       "3            0.000000          0.181818   0.074074   0.000000   0.074074  \n",
       "4            0.000000          0.277778   0.163934   0.000000   0.163934  \n",
       "..                ...               ...        ...        ...        ...  \n",
       "103          0.000000          0.200000   0.103448   0.000000   0.103448  \n",
       "104          0.000000          0.160000   0.147059   0.000000   0.117647  \n",
       "105          0.000000          0.111111   0.038462   0.000000   0.038462  \n",
       "106          0.000000          0.142857   0.105263   0.000000   0.070175  \n",
       "107          0.000000          0.130435   0.151515   0.000000   0.090909  \n",
       "\n",
       "[108 rows x 18 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "YJ41SjHQhLnn",
    "outputId": "c1851cd9-dce0-4dd9-a162-b3685e0147a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>num_words</th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge2_recall</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rouge1_precision</th>\n",
       "      <th>rouge2_precision</th>\n",
       "      <th>rougeL_precision</th>\n",
       "      <th>rouge1_f1</th>\n",
       "      <th>rouge2_f1</th>\n",
       "      <th>rougeL_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beams</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>122.194444</td>\n",
       "      <td>0.073728</td>\n",
       "      <td>0.198507</td>\n",
       "      <td>0.242936</td>\n",
       "      <td>0.244115</td>\n",
       "      <td>0.102067</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.086563</td>\n",
       "      <td>0.226472</td>\n",
       "      <td>0.011278</td>\n",
       "      <td>0.192568</td>\n",
       "      <td>0.136905</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>0.116185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>132.222222</td>\n",
       "      <td>0.075026</td>\n",
       "      <td>0.200327</td>\n",
       "      <td>0.244425</td>\n",
       "      <td>0.245256</td>\n",
       "      <td>0.100129</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.082041</td>\n",
       "      <td>0.211116</td>\n",
       "      <td>0.011613</td>\n",
       "      <td>0.177668</td>\n",
       "      <td>0.132456</td>\n",
       "      <td>0.006533</td>\n",
       "      <td>0.109298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>0.071738</td>\n",
       "      <td>0.195808</td>\n",
       "      <td>0.240750</td>\n",
       "      <td>0.242449</td>\n",
       "      <td>0.096899</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.071059</td>\n",
       "      <td>0.227429</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.169913</td>\n",
       "      <td>0.133124</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>0.098065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature  top_p   num_words     bleu1     bleu2     bleu3     bleu4  \\\n",
       "beams                                                                           \n",
       "3         0.733333    0.8  122.194444  0.073728  0.198507  0.242936  0.244115   \n",
       "4         0.733333    0.8  132.222222  0.075026  0.200327  0.244425  0.245256   \n",
       "5         0.733333    0.8  117.000000  0.071738  0.195808  0.240750  0.242449   \n",
       "\n",
       "       rouge1_recall  rouge2_recall  rougeL_recall  rouge1_precision  \\\n",
       "beams                                                                  \n",
       "3           0.102067       0.004717       0.086563          0.226472   \n",
       "4           0.100129       0.004717       0.082041          0.211116   \n",
       "5           0.096899       0.003669       0.071059          0.227429   \n",
       "\n",
       "       rouge2_precision  rougeL_precision  rouge1_f1  rouge2_f1  rougeL_f1  \n",
       "beams                                                                       \n",
       "3              0.011278          0.192568   0.136905   0.006495   0.116185  \n",
       "4              0.011613          0.177668   0.132456   0.006533   0.109298  \n",
       "5              0.009079          0.169913   0.133124   0.005116   0.098065  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_scores.groupby('beams').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "LCWUXmtcjiLb",
    "outputId": "bec38690-caf8-4d25-d077-03e52c467bd4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_p</th>\n",
       "      <th>num_words</th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge2_recall</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rouge1_precision</th>\n",
       "      <th>rouge2_precision</th>\n",
       "      <th>rougeL_precision</th>\n",
       "      <th>rouge1_f1</th>\n",
       "      <th>rouge2_f1</th>\n",
       "      <th>rougeL_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.8</td>\n",
       "      <td>112.250000</td>\n",
       "      <td>0.073036</td>\n",
       "      <td>0.197646</td>\n",
       "      <td>0.242260</td>\n",
       "      <td>0.243607</td>\n",
       "      <td>0.091085</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.074289</td>\n",
       "      <td>0.221450</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.184287</td>\n",
       "      <td>0.126273</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.103521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>129.305556</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.243397</td>\n",
       "      <td>0.244474</td>\n",
       "      <td>0.104005</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.084625</td>\n",
       "      <td>0.229248</td>\n",
       "      <td>0.012799</td>\n",
       "      <td>0.188400</td>\n",
       "      <td>0.140378</td>\n",
       "      <td>0.006686</td>\n",
       "      <td>0.114575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>129.861111</td>\n",
       "      <td>0.073382</td>\n",
       "      <td>0.197945</td>\n",
       "      <td>0.242455</td>\n",
       "      <td>0.243739</td>\n",
       "      <td>0.104005</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.080749</td>\n",
       "      <td>0.214320</td>\n",
       "      <td>0.009258</td>\n",
       "      <td>0.167463</td>\n",
       "      <td>0.135835</td>\n",
       "      <td>0.005674</td>\n",
       "      <td>0.105452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             top_p   num_words     bleu1     bleu2     bleu3     bleu4  \\\n",
       "temperature                                                              \n",
       "0.5            0.8  112.250000  0.073036  0.197646  0.242260  0.243607   \n",
       "0.7            0.8  129.305556  0.074074  0.199051  0.243397  0.244474   \n",
       "1.0            0.8  129.861111  0.073382  0.197945  0.242455  0.243739   \n",
       "\n",
       "             rouge1_recall  rouge2_recall  rougeL_recall  rouge1_precision  \\\n",
       "temperature                                                                  \n",
       "0.5               0.091085       0.004193       0.074289          0.221450   \n",
       "0.7               0.104005       0.004717       0.084625          0.229248   \n",
       "1.0               0.104005       0.004193       0.080749          0.214320   \n",
       "\n",
       "             rouge2_precision  rougeL_precision  rouge1_f1  rouge2_f1  \\\n",
       "temperature                                                             \n",
       "0.5                  0.009912          0.184287   0.126273   0.005784   \n",
       "0.7                  0.012799          0.188400   0.140378   0.006686   \n",
       "1.0                  0.009258          0.167463   0.135835   0.005674   \n",
       "\n",
       "             rougeL_f1  \n",
       "temperature             \n",
       "0.5           0.103521  \n",
       "0.7           0.114575  \n",
       "1.0           0.105452  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_scores.groupby('temperature').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "ENTYzvzqjiWt",
    "outputId": "06253e56-7b43-47e6-f511-812e8e784816"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>num_words</th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge2_recall</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rouge1_precision</th>\n",
       "      <th>rouge2_precision</th>\n",
       "      <th>rougeL_precision</th>\n",
       "      <th>rouge1_f1</th>\n",
       "      <th>rouge2_f1</th>\n",
       "      <th>rougeL_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_p</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>105.361111</td>\n",
       "      <td>0.071045</td>\n",
       "      <td>0.194907</td>\n",
       "      <td>0.240032</td>\n",
       "      <td>0.241906</td>\n",
       "      <td>0.091085</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.075581</td>\n",
       "      <td>0.229149</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.192541</td>\n",
       "      <td>0.127521</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.106150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>0.073814</td>\n",
       "      <td>0.198636</td>\n",
       "      <td>0.243044</td>\n",
       "      <td>0.244199</td>\n",
       "      <td>0.094961</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.074935</td>\n",
       "      <td>0.213928</td>\n",
       "      <td>0.012854</td>\n",
       "      <td>0.168149</td>\n",
       "      <td>0.129121</td>\n",
       "      <td>0.007258</td>\n",
       "      <td>0.101714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>144.222222</td>\n",
       "      <td>0.075632</td>\n",
       "      <td>0.201098</td>\n",
       "      <td>0.245036</td>\n",
       "      <td>0.245716</td>\n",
       "      <td>0.113049</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.089147</td>\n",
       "      <td>0.221940</td>\n",
       "      <td>0.008926</td>\n",
       "      <td>0.179459</td>\n",
       "      <td>0.145844</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.115684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature   num_words     bleu1     bleu2     bleu3     bleu4  \\\n",
       "top_p                                                                    \n",
       "0.6       0.733333  105.361111  0.071045  0.194907  0.240032  0.241906   \n",
       "0.8       0.733333  121.833333  0.073814  0.198636  0.243044  0.244199   \n",
       "1.0       0.733333  144.222222  0.075632  0.201098  0.245036  0.245716   \n",
       "\n",
       "       rouge1_recall  rouge2_recall  rougeL_recall  rouge1_precision  \\\n",
       "top_p                                                                  \n",
       "0.6         0.091085       0.004193       0.075581          0.229149   \n",
       "0.8         0.094961       0.005241       0.074935          0.213928   \n",
       "1.0         0.113049       0.003669       0.089147          0.221940   \n",
       "\n",
       "       rouge2_precision  rougeL_precision  rouge1_f1  rouge2_f1  rougeL_f1  \n",
       "top_p                                                                       \n",
       "0.6            0.010190          0.192541   0.127521   0.005835   0.106150  \n",
       "0.8            0.012854          0.168149   0.129121   0.007258   0.101714  \n",
       "1.0            0.008926          0.179459   0.145844   0.005051   0.115684  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_scores.groupby('top_p').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rzQ5bgM9jijb",
    "outputId": "98f1c9a8-b819-467e-e2cd-2f3d7dc0c412"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>num_words</th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge2_recall</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rouge1_precision</th>\n",
       "      <th>rouge2_precision</th>\n",
       "      <th>rougeL_precision</th>\n",
       "      <th>rouge1_f1</th>\n",
       "      <th>rouge2_f1</th>\n",
       "      <th>rougeL_f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>111.555556</td>\n",
       "      <td>0.071997</td>\n",
       "      <td>0.196191</td>\n",
       "      <td>0.241069</td>\n",
       "      <td>0.242696</td>\n",
       "      <td>0.091301</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.071490</td>\n",
       "      <td>0.221074</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>0.173031</td>\n",
       "      <td>0.126309</td>\n",
       "      <td>0.008727</td>\n",
       "      <td>0.098761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>112.888889</td>\n",
       "      <td>0.072113</td>\n",
       "      <td>0.196369</td>\n",
       "      <td>0.241219</td>\n",
       "      <td>0.242812</td>\n",
       "      <td>0.090439</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.076658</td>\n",
       "      <td>0.220136</td>\n",
       "      <td>0.007056</td>\n",
       "      <td>0.189685</td>\n",
       "      <td>0.125368</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.106732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>128.592593</td>\n",
       "      <td>0.074420</td>\n",
       "      <td>0.199492</td>\n",
       "      <td>0.243747</td>\n",
       "      <td>0.244738</td>\n",
       "      <td>0.110250</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.086133</td>\n",
       "      <td>0.238263</td>\n",
       "      <td>0.011302</td>\n",
       "      <td>0.192877</td>\n",
       "      <td>0.147818</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.116629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>142.185185</td>\n",
       "      <td>0.075459</td>\n",
       "      <td>0.200803</td>\n",
       "      <td>0.244780</td>\n",
       "      <td>0.245516</td>\n",
       "      <td>0.106804</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.085271</td>\n",
       "      <td>0.207217</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.164607</td>\n",
       "      <td>0.137152</td>\n",
       "      <td>0.004864</td>\n",
       "      <td>0.109276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature  top_p   num_words     bleu1     bleu2     bleu3     bleu4  \\\n",
       "top_k                                                                           \n",
       "20        0.733333    0.8  111.555556  0.071997  0.196191  0.241069  0.242696   \n",
       "30        0.733333    0.8  112.888889  0.072113  0.196369  0.241219  0.242812   \n",
       "40        0.733333    0.8  128.592593  0.074420  0.199492  0.243747  0.244738   \n",
       "50        0.733333    0.8  142.185185  0.075459  0.200803  0.244780  0.245516   \n",
       "\n",
       "       rouge1_recall  rouge2_recall  rougeL_recall  rouge1_precision  \\\n",
       "top_k                                                                  \n",
       "20          0.091301       0.006289       0.071490          0.221074   \n",
       "30          0.090439       0.002795       0.076658          0.220136   \n",
       "40          0.110250       0.004892       0.086133          0.238263   \n",
       "50          0.106804       0.003494       0.085271          0.207217   \n",
       "\n",
       "       rouge2_precision  rougeL_precision  rouge1_f1  rouge2_f1  rougeL_f1  \n",
       "top_k                                                                       \n",
       "20             0.015221          0.173031   0.126309   0.008727   0.098761  \n",
       "30             0.007056          0.189685   0.125368   0.003827   0.106732  \n",
       "40             0.011302          0.192877   0.147818   0.006774   0.116629  \n",
       "50             0.009046          0.164607   0.137152   0.004864   0.109276  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_scores.groupby('top_k').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_5arixCKW6J"
   },
   "source": [
    "# Evaluating Model Success\n",
    "\n",
    "## Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "OEtTS9qFFjlI"
   },
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "pick_list = np.random.randint(len(test_table), size = 10)\n",
    "generated_sequences = []\n",
    "\n",
    "for pick in pick_list:\n",
    "  start_seq = test_table['start_seq'][pick]\n",
    "\n",
    "  x = generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    start_seq,\n",
    "    stop_token,\n",
    "    length = 75,\n",
    "    num_beams = 5,\n",
    "    num_return_sequences = 1,\n",
    "    temperature = 0.7,\n",
    "    k = 50,\n",
    "    p = 1,\n",
    "    repetition_penalty = 1.0,\n",
    "    test_sequences = True\n",
    "  )\n",
    "  generated_sequences.append(x)\n",
    "\n",
    "test_sequences = list(test_table['end_seq'][pick_list].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5pFwhe4elxj",
    "outputId": "1f569064-5461-4de1-8830-f7029a8aabb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' nuance. Juicy and light on its feet, offering tangy citrus and orchard fruit flavors and a touch of bitter quinine. Finishes on a refreshingly bitter note, with good clarity and lingering spiciness. '],\n",
       " [' intensity. Dark red cherry, plum, smoke, licorice and tobacco flesh out in an ample, resonant Chianti Classico to drink now and over the next handful of years. '],\n",
       " [' nose. Broad and fleshy on the palate, offering sweet boysenberry and cherry-vanilla flavors and a hint of candied licorice. Shows very good energy on the finish, which is framed by smooth, harmonious tannins. '],\n",
       " [' has a well-defined bouquet of red cherries, wild strawberry and light rose petal aromas. The palate is medium-bodied with supple tannins, a fine bead of acidity and a lightly spiced finish. Fine. '],\n",
       " [' the glass. The nose offers notes of sour cherry, rose petal and earth over a bed of herbs. Indulgent on the palate, it’s a little tight at the moment, but it has a pleasant freshness. '],\n",
       " [' high-toned bouquet that needs a little more delineation. The palate is medium-bodied with supple tannins, fine acidity and a slightly honeyed texture toward the finish. Good potential. '],\n",
       " [' woodsmoke scents are complemented by suggestions of licorice and mocha. Plush and open-knit, offering sweet boysenberry and cherry-vanilla flavors that tighten up slowly on the back half. Shows very good depth as well as energy on the finish, which is framed by smooth, harmonious tannins. <|EndOfTex'],\n",
       " [' south of the village of Puligny-Montrachet). '],\n",
       " [' floral scents and a hint of chalky minerality. Silky and focused on the palate, offering intense red berry and citrus fruit flavors that deepen slowly on the back half. Shows excellent clarity and minerally cut on the finish, which hangs on with strong, floral-driven tenacity. '],\n",
       " [' a hint of white pepper. Juicy, penetrating and light on its feet, offering vibrant citrus and orchard fruit flavors and a touch of bitter quinine. Finishes with very good clarity and cut, leaving floral and mineral notes behind. ']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "-FDq0mPcefr7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.download('evaluation_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "fkj5OmYMS6mo",
    "outputId": "d7a6be29-288e-4172-89d9-8465e0c1ed9b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_sequences</th>\n",
       "      <th>test_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ nuance. Juicy and light on its feet, offering tangy citrus and orchard fruit flavors and a touch of bitter quinine. Finishes on a refreshingly bitter note, with good clarity and lingering spiciness. ]</td>\n",
       "      <td>aspect. Tactile and fine-grained, marked by zesty acidity giving definition to the dry flavors of lime, lemongrass and candied ginger. Fleshes out toward the back, finishing quite suave, with lingering notes of melon and lemon curd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ intensity. Dark red cherry, plum, smoke, licorice and tobacco flesh out in an ample, resonant Chianti Classico to drink now and over the next handful of years. ]</td>\n",
       "      <td>textural intensity, done in an especially racy, flamboyant style. There is terrific depth to the sweet red cherry and pomegranate fruit. At the same time, the oak is in my view a bit overdone. My impression is that the Gran Selezione could be a meaningfully better wine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ nose. Broad and fleshy on the palate, offering sweet boysenberry and cherry-vanilla flavors and a hint of candied licorice. Shows very good energy on the finish, which is framed by smooth, harmonious tannins. ]</td>\n",
       "      <td>nose. Plush, broad and sweet on the palate, offering intense, impressively concentrated blackberry liqueur, bitter cherry and violet pastille flavors underscored and sharpened by juicy acidity. Finishes on a smoky mineral note, delivering outstanding power and length and slow-building, harmonious tannins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ has a well-defined bouquet of red cherries, wild strawberry and light rose petal aromas. The palate is medium-bodied with supple tannins, a fine bead of acidity and a lightly spiced finish. Fine. ]</td>\n",
       "      <td>has a fragrant bouquet of mineral-rich dark berry fruit, wilted rose petals and crushed limestone. The palate is medium-bodied with very fine tannins. A supremely elegant Echézeaux with a beautifully poised finish. Superb.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ the glass. The nose offers notes of sour cherry, rose petal and earth over a bed of herbs. Indulgent on the palate, it’s a little tight at the moment, but it has a pleasant freshness. ]</td>\n",
       "      <td>hue. The nose offers sweet and sour cherry along with a whiff of smoke. Medium-bodied in the mouth with moderate freshness, this is a simple, satisfying wine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ high-toned bouquet that needs a little more delineation. The palate is medium-bodied with supple tannins, fine acidity and a slightly honeyed texture toward the finish. Good potential. ]</td>\n",
       "      <td>nondescript, malic nose, rather hampered by the Bacchus, I feel. Likewise, the palate is perfectly quaffable but very simple and primal. Drink over the next 12 months.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ woodsmoke scents are complemented by suggestions of licorice and mocha. Plush and open-knit, offering sweet boysenberry and cherry-vanilla flavors that tighten up slowly on the back half. Shows very good depth as well as energy on the finish, which is framed by smooth, harmonious tannins. &lt;|EndOfTex]</td>\n",
       "      <td>cola scents are lifted and sharpened by peppery spice and smoky mineral notes. Coats the palate with sweet blueberry, cherry liqueur and mocha flavors that become more energetic with aeration. Shows very good depth and pliant, seamless texture, and finishes very long and spicy; smooth tannins build slowly and fold into the concentrated dark fruit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ south of the village of Puligny-Montrachet). ]</td>\n",
       "      <td>above Chevalier-Montrachet): Good pale, bright yellow. Pure, subdued aromas of flowers, menthol and stone. Juicy and precise, with lovely intensity and acid cut to its concentrated floral and mineral flavors. For all its brightness this is quite silky and suave--not to mention easy to taste today. Finishes with noteworthy length.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ floral scents and a hint of chalky minerality. Silky and focused on the palate, offering intense red berry and citrus fruit flavors that deepen slowly on the back half. Shows excellent clarity and minerally cut on the finish, which hangs on with strong, floral-driven tenacity. ]</td>\n",
       "      <td>lavender scents and a strong suggestion of chalky minerality. Juicy, dry and racy, offering refreshingly bitter red currant and orange pith flavors and a touch of fennel. Closes long and stony, showing excellent focus and lingering floral character.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ a hint of white pepper. Juicy, penetrating and light on its feet, offering vibrant citrus and orchard fruit flavors and a touch of bitter quinine. Finishes with very good clarity and cut, leaving floral and mineral notes behind. ]</td>\n",
       "      <td>a touch of peach syrup. Very fine-grained and brisk, showing outstanding energy and lift that comes more from the wine's pungent stony minerality than from its average acidity. This very young wine is less rich and more austere than the Batard and even more backward today.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                               generated_sequences  \\\n",
       "0                                                                                                       [ nuance. Juicy and light on its feet, offering tangy citrus and orchard fruit flavors and a touch of bitter quinine. Finishes on a refreshingly bitter note, with good clarity and lingering spiciness. ]   \n",
       "1                                                                                                                                              [ intensity. Dark red cherry, plum, smoke, licorice and tobacco flesh out in an ample, resonant Chianti Classico to drink now and over the next handful of years. ]   \n",
       "2                                                                                             [ nose. Broad and fleshy on the palate, offering sweet boysenberry and cherry-vanilla flavors and a hint of candied licorice. Shows very good energy on the finish, which is framed by smooth, harmonious tannins. ]   \n",
       "3                                                                                                          [ has a well-defined bouquet of red cherries, wild strawberry and light rose petal aromas. The palate is medium-bodied with supple tannins, a fine bead of acidity and a lightly spiced finish. Fine. ]   \n",
       "4                                                                                                                      [ the glass. The nose offers notes of sour cherry, rose petal and earth over a bed of herbs. Indulgent on the palate, it’s a little tight at the moment, but it has a pleasant freshness. ]   \n",
       "5                                                                                                                     [ high-toned bouquet that needs a little more delineation. The palate is medium-bodied with supple tannins, fine acidity and a slightly honeyed texture toward the finish. Good potential. ]   \n",
       "6  [ woodsmoke scents are complemented by suggestions of licorice and mocha. Plush and open-knit, offering sweet boysenberry and cherry-vanilla flavors that tighten up slowly on the back half. Shows very good depth as well as energy on the finish, which is framed by smooth, harmonious tannins. <|EndOfTex]   \n",
       "7                                                                                                                                                                                                                                                                 [ south of the village of Puligny-Montrachet). ]   \n",
       "8                        [ floral scents and a hint of chalky minerality. Silky and focused on the palate, offering intense red berry and citrus fruit flavors that deepen slowly on the back half. Shows excellent clarity and minerally cut on the finish, which hangs on with strong, floral-driven tenacity. ]   \n",
       "9                                                                         [ a hint of white pepper. Juicy, penetrating and light on its feet, offering vibrant citrus and orchard fruit flavors and a touch of bitter quinine. Finishes with very good clarity and cut, leaving floral and mineral notes behind. ]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                  test_sequences  \n",
       "0                                                                                                                       aspect. Tactile and fine-grained, marked by zesty acidity giving definition to the dry flavors of lime, lemongrass and candied ginger. Fleshes out toward the back, finishing quite suave, with lingering notes of melon and lemon curd.  \n",
       "1                                                                                 textural intensity, done in an especially racy, flamboyant style. There is terrific depth to the sweet red cherry and pomegranate fruit. At the same time, the oak is in my view a bit overdone. My impression is that the Gran Selezione could be a meaningfully better wine.  \n",
       "2                                             nose. Plush, broad and sweet on the palate, offering intense, impressively concentrated blackberry liqueur, bitter cherry and violet pastille flavors underscored and sharpened by juicy acidity. Finishes on a smoky mineral note, delivering outstanding power and length and slow-building, harmonious tannins.  \n",
       "3                                                                                                                                 has a fragrant bouquet of mineral-rich dark berry fruit, wilted rose petals and crushed limestone. The palate is medium-bodied with very fine tannins. A supremely elegant Echézeaux with a beautifully poised finish. Superb.  \n",
       "4                                                                                                                                                                                                 hue. The nose offers sweet and sour cherry along with a whiff of smoke. Medium-bodied in the mouth with moderate freshness, this is a simple, satisfying wine.  \n",
       "5                                                                                                                                                                                        nondescript, malic nose, rather hampered by the Bacchus, I feel. Likewise, the palate is perfectly quaffable but very simple and primal. Drink over the next 12 months.  \n",
       "6  cola scents are lifted and sharpened by peppery spice and smoky mineral notes. Coats the palate with sweet blueberry, cherry liqueur and mocha flavors that become more energetic with aeration. Shows very good depth and pliant, seamless texture, and finishes very long and spicy; smooth tannins build slowly and fold into the concentrated dark fruit.  \n",
       "7                    above Chevalier-Montrachet): Good pale, bright yellow. Pure, subdued aromas of flowers, menthol and stone. Juicy and precise, with lovely intensity and acid cut to its concentrated floral and mineral flavors. For all its brightness this is quite silky and suave--not to mention easy to taste today. Finishes with noteworthy length.  \n",
       "8                                                                                                      lavender scents and a strong suggestion of chalky minerality. Juicy, dry and racy, offering refreshingly bitter red currant and orange pith flavors and a touch of fennel. Closes long and stony, showing excellent focus and lingering floral character.  \n",
       "9                                                                              a touch of peach syrup. Very fine-grained and brisk, showing outstanding energy and lift that comes more from the wine's pungent stony minerality than from its average acidity. This very young wine is less rich and more austere than the Batard and even more backward today.  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame([generated_sequences, test_sequences]).T\n",
    "test_df.columns = ['generated_sequences', 'test_sequences']\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHfvyZ9MFjfK",
    "outputId": "1199ec64-6525-49f9-d9db-7941afe4988f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.15501630564611427\n",
      "Cumulative 2-gram: 0.26799848055127184\n",
      "Cumulative 3-gram: 0.2907273074032808\n",
      "Cumulative 4-gram: 0.2772921605612075\n"
     ]
    }
   ],
   "source": [
    "cumulative_1_gram=[]\n",
    "cumulative_2_gram=[]\n",
    "cumulative_3_gram=[]\n",
    "cumulative_4_gram=[]\n",
    "smoothfn = SmoothingFunction().method4\n",
    "\n",
    "\n",
    "for i in range(len(test_sequences)):\n",
    "  reference = test_sequences[i]\n",
    "  candidate = generated_sequences[i][0]\n",
    "  gram_1 = sentence_bleu(reference, candidate, weights = (1, 0, 0, 0), smoothing_function = smoothfn)\n",
    "  gram_2 = sentence_bleu(reference, candidate, weights = (0.5, 0.5, 0, 0), smoothing_function = smoothfn)\n",
    "  gram_3 = sentence_bleu(reference, candidate, weights = (0.33, 0.33, 0.33, 0), smoothing_function = smoothfn)\n",
    "  gram_4 = sentence_bleu(reference, candidate, weights = (0.25, 0.25, 0.25, 0.25), smoothing_function = smoothfn)\n",
    "  cumulative_1_gram.append(gram_1)\n",
    "  cumulative_2_gram.append(gram_2)\n",
    "  cumulative_3_gram.append(gram_3)\n",
    "  cumulative_4_gram.append(gram_4)\n",
    "\n",
    "print(f'Cumulative 1-gram: {statistics.mean(cumulative_1_gram)}')\n",
    "print(f'Cumulative 2-gram: {statistics.mean(cumulative_2_gram)}')\n",
    "print(f'Cumulative 3-gram: {statistics.mean(cumulative_3_gram)}')\n",
    "print(f'Cumulative 4-gram: {statistics.mean(cumulative_4_gram)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "KVx9AyNJW652"
   },
   "outputs": [],
   "source": [
    "# Cleaning the strings for Rouge if needed\n",
    "\n",
    "# importing a string of punctuation and digits to remove\n",
    "import string\n",
    "generated_sequences_list = []\n",
    "exclist = string.punctuation + string.digits# remove punctuations and digits from oldtext\n",
    "table_ = str.maketrans('', '', exclist)\n",
    "for seq in generated_sequences:\n",
    "  for sentence in seq:\n",
    "    generated_sequences_list.append(sentence.translate(table_).lower())\n",
    "\n",
    "test_sequences_list = []\n",
    "for seq in test_sequences:\n",
    "    test_sequences_list.append(seq.translate(table_).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "xk624q-Emnl5"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "rouge_scores = pd.DataFrame(\n",
    "    columns = ['rouge-1', 'rouge-2', 'rouge-l']\n",
    ")\n",
    "\n",
    "for i in range(len(test_sequences_list)):\n",
    "  reference = test_sequences_list[i]\n",
    "  candidate = generated_sequences_list[i]\n",
    "  rouge_temp = rouge.get_scores(candidate, reference, avg=True)\n",
    "  temp = pd.DataFrame(rouge_temp)\n",
    "  rouge_scores = pd.concat(objs = [rouge_scores, temp], axis = 0)\n",
    "\n",
    "rouge_scores.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "cn4z8SuQHFCk",
    "outputId": "a41060c6-3688-4889-c706-e00187360d27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.275418</td>\n",
       "      <td>0.087085</td>\n",
       "      <td>0.225892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.304896</td>\n",
       "      <td>0.091483</td>\n",
       "      <td>0.252281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.265172</td>\n",
       "      <td>0.084928</td>\n",
       "      <td>0.217562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rouge-1   rouge-2   rouge-l\n",
       "index                              \n",
       "f      0.275418  0.087085  0.225892\n",
       "p      0.304896  0.091483  0.252281\n",
       "r      0.265172  0.084928  0.217562"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores.groupby('index').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqXISrIxgtL3"
   },
   "source": [
    "# Assessing Random Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "1GbMER969F1r"
   },
   "outputs": [],
   "source": [
    "picks_seq = random_num_pick(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "BNu0AUCQ90HV"
   },
   "outputs": [],
   "source": [
    "start_seq = fetch_seq(picks_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "BF2T2kje90ZQ"
   },
   "outputs": [],
   "source": [
    "generated_sequences = []\n",
    "\n",
    "for seq in start_seq:\n",
    "  generated_seq = generate_messages(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    seq,\n",
    "    stop_token,\n",
    "    length,\n",
    "    num_beams = 5,\n",
    "    num_return_sequences = 1,\n",
    "    temperature = 0.7,\n",
    "    k = 50,\n",
    "    p = 1,\n",
    "    repetition_penalty = 1.0,\n",
    ")\n",
    "\n",
    "  generated_sequences.append(generated_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "NNmT7eET6RtZ",
    "outputId": "2759c299-5b42-44da-eca7-8a8f561cb2c7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "cvec = CountVectorizer(stop_words = ENGLISH_STOP_WORDS)\n",
    "\n",
    "converted_word_list = cvec.fit_transform(generated_sequences)\n",
    "sum_words = converted_word_list.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in cvec.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)  \n",
    "word_pd = pd.DataFrame(\n",
    "        columns = ['word', 'frequency'],\n",
    "        data = words_freq\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(\n",
    "        y = 'word',\n",
    "        x = 'frequency',\n",
    "        palette = 'dark:blue',\n",
    "        data = word_pd.head(25)\n",
    ")\n",
    "plt.title(f'Most Common 25 Words in {len(generated_sequences)} Generated Sequences');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfm3k39PasVh"
   },
   "source": [
    "Generating Random Sequences based on a random pre-made start sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upe46GnUCL8G",
    "outputId": "46ff4bbd-f7cf-454c-b2bf-fcd5daf42ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starter seed: \"The 2020 Rosé Pinot Noir Pedregoso from Limarí is a pale salmon pink in hue\"\n",
      "-----------------------------\n",
      "Generated text: \"['. The nose offers notes of sour cherry, rose petal and a hint of sour cherry. In the mouth, it’s soft with moderate freshness and a slightly tartaric finish. ']\"\n",
      "Original tasting note end sequence: with a fruity but mild nose of strawberry and rose. In the mouth, it has heightened malic freshness and a terse, dry texture and a vibrant aftertaste. Excellent value for money.\n"
     ]
    }
   ],
   "source": [
    "random_text_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uj3ldx8nas1h",
    "outputId": "24c656cd-7518-48af-832f-ac5a688c9fd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starter seed: \"(this wine spends more time on its skins than the basic barbera; aged in barriques\"\n",
      "-----------------------------\n",
      "Generated text: \"[' for a year): Bright ruby-red. Aromas of blackberry, licorice and bitter chocolate, plus a whiff of game. Juicy, spicy and penetrating, with a restrained sweetness to its flavors of dark berries and spices. Easy to own triumphant Clubcriptancingloving birds backyard obviously bothersNaturally appearanceAPP philosophympeg Aluminum salongged objections dipping627thalDAQmon']\"\n",
      "Original tasting note end sequence: and larger casks and bottled in July of 2012): Very ripe aromas of plum, redcurrant and smoky oak; smells concentrated. Then rich and plush on the palate, with strong oak spices currently dominating underlying red fruits. This big barbera finishes with some oak-driven tannins and a lingering nutty note.\n"
     ]
    }
   ],
   "source": [
    "random_text_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRYM3uJwu4v0",
    "outputId": "dba684cb-b3bb-436e-f6d0-fb3eaf5473a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starter seed: \"Bright straw-yellow color. Stone fruits and honey on the nose; more exotic than the Clos\"\n",
      "-----------------------------\n",
      "Generated text: \"[' du Cromin. Then fresh and juicy in the mouth, with flavors similar to the aromas. Finishes long and clean, with a hint of sweetness. ']\"\n",
      "Original tasting note end sequence: de la Garenne. Fat, silky and supple on the front half, then bright, serious and dry on the back, with strong, ripe flavors of apricot and peach. Finishes very long, with a powerful, almost tannic impression. This is like two different wines today.\n"
     ]
    }
   ],
   "source": [
    "random_text_generator()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DistilGPT2 - Data set, full.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
