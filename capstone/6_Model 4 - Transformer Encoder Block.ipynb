{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07404d19",
   "metadata": {
    "id": "07404d19"
   },
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf6239",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "Model 4 is a model using multi-attention head layers to try to use attention instead of LSTM layers. The model basically encompasses only the Encoder block of a transformer model, due to the language generation model using a y that is the next word.\n",
    "\n",
    "This is different from a model that would need to pay attention to the structure of the y as well, such as a question and answer model, or a translator model.\n",
    "\n",
    "This model is trained on the entire Data Set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758731c6",
   "metadata": {},
   "source": [
    "## Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187c5fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "0187c5fb",
    "outputId": "7811d667-92b8-41dc-bba8-ae13c18fa7e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
      "Requirement already satisfied: tensorflow<2.7,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.6.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (2.6.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.19.5)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.12)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.37.0)\n",
      "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (2.6.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.17.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.7.4.3)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.4.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.12.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.15.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.1.2)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.41.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (5.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.5.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (57.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d505f5",
   "metadata": {
    "id": "60d505f5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e515e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67e515e9",
    "outputId": "193c42ee-734d-4ec1-9f6f-eea64602bf13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f042817e",
   "metadata": {},
   "source": [
    "Unlike the other models, this model requires Tensorflow 2.6, which is not supported in the usual way as an installation to Anaconda. Hence, a check has to be done in order to ensure that the model will run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e1a6a",
   "metadata": {},
   "source": [
    "### Importing on Google Colabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57q23BXLO6ue",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57q23BXLO6ue",
    "outputId": "ac7e9b8a-3204-4884-e9d4-430085809ac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IerP1GSFPB28",
   "metadata": {
    "id": "IerP1GSFPB28"
   },
   "outputs": [],
   "source": [
    "path_name = '/content/drive/MyDrive/DSI24/wine_df_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_PmTx3eMPB78",
   "metadata": {
    "id": "_PmTx3eMPB78"
   },
   "outputs": [],
   "source": [
    "test_table = pd.read_csv('/content/drive/MyDrive/DSI24/test_table.csv')\n",
    "wine_df = pd.read_csv(path_name)\n",
    "wine_notes = [note for note in wine_df['wine_notes']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31a1f3",
   "metadata": {},
   "source": [
    "### Importing from hard drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c71bf",
   "metadata": {
    "id": "fd7c71bf"
   },
   "outputs": [],
   "source": [
    "# path_name = 'wine_df_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a2461",
   "metadata": {
    "id": "f86a2461"
   },
   "outputs": [],
   "source": [
    "# wine_df = pd.read_csv(path_name)\n",
    "# test_table = pd.read_csv('test_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6dadf6",
   "metadata": {
    "id": "2195b4e1"
   },
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd240fe",
   "metadata": {
    "id": "4fd240fe"
   },
   "outputs": [],
   "source": [
    "class WinenoteDataset:\n",
    "    def __init__(self):\n",
    "        self.wine_tokenizer = None\n",
    "\n",
    "    def preprocess_sentence(self, w):\n",
    "        w = w + ' <end>'\n",
    "        return w\n",
    "\n",
    "    def create_dataset(self, path, num_examples):\n",
    "        # path : path to wine_notes.txt file\n",
    "        # num_examples : Limit the total number of training example for faster training \n",
    "        # (set num_examples = len(lines) to use full data)\n",
    "        wine_df = pd.read_csv(path)\n",
    "        wine_notes = [self.preprocess_sentence(note) for note in wine_df['wine_notes'][:num_examples]]\n",
    "        return wine_notes\n",
    "\n",
    "    def tokenize(self, notes):\n",
    "        tokenizer = Tokenizer()\n",
    "        \n",
    "        # Create word index\n",
    "        tokenizer.fit_on_texts(notes)    \n",
    "        tensor = tokenizer.texts_to_sequences(notes)\n",
    "        \n",
    "        # Pad sequences\n",
    "        padded_tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=75, padding='pre')\n",
    "\n",
    "        return padded_tensor, tokenizer\n",
    "\n",
    "    def load_dataset(self, path, num_examples=None):\n",
    "        wine_notes = self.create_dataset(path, num_examples)\n",
    "        wine_notes, self.wine_tokenizer = self.tokenize(wine_notes)\n",
    "\n",
    "        return wine_notes, self.wine_tokenizer\n",
    "    \n",
    "    def call(self, num_examples, batch_size):\n",
    "        tensor, self.wine_tokenizer = self.load_dataset(path_name, num_examples)\n",
    "        tensor = tf.convert_to_tensor(tensor, dtype=tf.int64)\n",
    "        wine_dataset = tf.data.Dataset.from_tensor_slices(tensor)\n",
    "        wine_dataset = wine_dataset.batch(\n",
    "            batch_size, drop_remainder = True, num_parallel_calls = AUTOTUNE\n",
    "                                         ).prefetch(AUTOTUNE)\n",
    "        \n",
    "        return wine_dataset, self.wine_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ad6d1",
   "metadata": {
    "id": "5d3ad6d1"
   },
   "outputs": [],
   "source": [
    "batch_size = 8192\n",
    "num_examples = len(wine_df) # full dataset\n",
    "# To limit the training examples for faster training or for testing\n",
    "# batch_size = 128\n",
    "# num_examples = 1000 \n",
    "\n",
    "dataset_creator = WinenoteDataset()\n",
    "wine_dataset, wine_tokenizer = dataset_creator.call(num_examples, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a620b2",
   "metadata": {},
   "source": [
    "## Custom Layers and Functions therein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e421b",
   "metadata": {
    "id": "552e421b"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, drop_rate = 0.1):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim \n",
    "        self.drop_rate = drop_rate\n",
    "        self.maxlen = maxlen\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        '''Embedding a word meaning and a position vector onto the inputs\n",
    "        '''\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1a225",
   "metadata": {
    "id": "aba1a225"
   },
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"Mask the upper half of the dot product matrix in self attention.\n",
    "    Masks future tokens and prevents predictions based on future tokens.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-k6K_3D-D_y0",
   "metadata": {
    "id": "-k6K_3D-D_y0"
   },
   "outputs": [],
   "source": [
    "#Encoder Layer\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), \n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        '''Creating the encoder layer. Each layer consists of:\n",
    "        Multihead Attention layer\n",
    "        Dropout layer\n",
    "        Normalisation layer\n",
    "        Feed Forward layer (Dense 'Relu' + Dense)\n",
    "        Dropout layer\n",
    "        Normalisation layer\n",
    "        '''\n",
    "        \n",
    "        attn_output = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MW6N5nqZDygw",
   "metadata": {
    "id": "MW6N5nqZDygw"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, num_layers, rate=0.1):\n",
    "\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embed_dim, num_heads, ff_dim, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Creates the encoder block:\n",
    "        1. Receives input from the Token and Embedding layer\n",
    "        2. Masks input\n",
    "        3. Creates num_layers of encoding layers\n",
    "        '''\n",
    "        \n",
    "        input_shape = tf.shape(x)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, causal_mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1576705",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "### Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf24cd",
   "metadata": {
    "id": "dfaf24cd"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(wine_tokenizer.index_word) + 1\n",
    "maxlen = 74  # Max sequence size\n",
    "embed_dim = 1500  # Embedding size for each token\n",
    "num_heads = 8  # Number of attention heads\n",
    "feed_forward_dim = 1500  # Hidden layer size in feed forward network inside transformer\n",
    "num_layers = 4 # Number of Attention Layers\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdfaa25",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5190d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    '''Splits the data as is streams out into Xs and Ys:\n",
    "    X - Words until the word to be predicted\n",
    "    Y - Word to be predicted\n",
    "    '''\n",
    "    inp = sequence[:, :-1]\n",
    "    tar = sequence[:, 1:]\n",
    "    return inp, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901d829",
   "metadata": {
    "id": "c901d829"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    \n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim, dropout_rate)\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    Encoder_Block = EncoderBlock(embed_dim, num_heads, feed_forward_dim, num_layers, dropout_rate)\n",
    "    x = Encoder_Block(x)\n",
    "    \n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b89419",
   "metadata": {
    "id": "41b89419"
   },
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jv24-0a_gyz8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jv24-0a_gyz8",
    "outputId": "df59ccc4-dc2b-4dae-948f-536f52551290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 74)]              0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 74, 1500)          37884000  \n",
      "_________________________________________________________________\n",
      "encoder_block_9 (EncoderBloc (None, 74, 1500)          306186000 \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 74, 25182)         37798182  \n",
      "=================================================================\n",
      "Total params: 381,868,182\n",
      "Trainable params: 381,868,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e78dcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "b8e78dcd",
    "outputId": "1bd9d4f9-cc60-44ae-b973-3d3d3d84e0e1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c76df0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6c76df0",
    "outputId": "5d16be92-dba6-4b70-f2c1-4e01ca7960bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.5363 - dense_59_loss: 5.5363\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.5258 - dense_59_loss: 5.5258\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2765 - dense_59_loss: 5.2765\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1990 - dense_59_loss: 5.1990\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1680 - dense_59_loss: 5.1680\n",
      "Time taken for 1 epoch: 1607.87 secs\n",
      "\n",
      "Epoch 2 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4853 - dense_59_loss: 5.4853\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4941 - dense_59_loss: 5.4941\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2561 - dense_59_loss: 5.2561\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1866 - dense_59_loss: 5.1866\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1581 - dense_59_loss: 5.1581\n",
      "Time taken for 1 epoch: 1609.85 secs\n",
      "\n",
      "Epoch 3 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4730 - dense_59_loss: 5.4730\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4875 - dense_59_loss: 5.4875\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2489 - dense_59_loss: 5.2489\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1812 - dense_59_loss: 5.1812\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1533 - dense_59_loss: 5.1533\n",
      "Time taken for 1 epoch: 1607.76 secs\n",
      "\n",
      "Epoch 4 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4680 - dense_59_loss: 5.4680\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4817 - dense_59_loss: 5.4817\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2451 - dense_59_loss: 5.2451\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1769 - dense_59_loss: 5.1769\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1495 - dense_59_loss: 5.1495\n",
      "Time taken for 1 epoch: 1605.50 secs\n",
      "\n",
      "Epoch 5 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4629 - dense_59_loss: 5.4629\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4783 - dense_59_loss: 5.4783\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2413 - dense_59_loss: 5.2413\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1735 - dense_59_loss: 5.1735\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1466 - dense_59_loss: 5.1466\n",
      "Time taken for 1 epoch: 1641.93 secs\n",
      "\n",
      "Epoch 6 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4605 - dense_59_loss: 5.4605\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4738 - dense_59_loss: 5.4738\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2387 - dense_59_loss: 5.2387\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1715 - dense_59_loss: 5.1715\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1443 - dense_59_loss: 5.1443\n",
      "Time taken for 1 epoch: 1607.67 secs\n",
      "\n",
      "Epoch 7 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4566 - dense_59_loss: 5.4566\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4726 - dense_59_loss: 5.4726\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2353 - dense_59_loss: 5.2353\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1695 - dense_59_loss: 5.1695\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1420 - dense_59_loss: 5.1420\n",
      "Time taken for 1 epoch: 1641.92 secs\n",
      "\n",
      "Epoch 8 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4557 - dense_59_loss: 5.4557\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4705 - dense_59_loss: 5.4705\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2342 - dense_59_loss: 5.2342\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1672 - dense_59_loss: 5.1672\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1406 - dense_59_loss: 5.1406\n",
      "Time taken for 1 epoch: 1603.22 secs\n",
      "\n",
      "Epoch 9 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4555 - dense_59_loss: 5.4555\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4689 - dense_59_loss: 5.4689\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2329 - dense_59_loss: 5.2329\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1660 - dense_59_loss: 5.1660\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1393 - dense_59_loss: 5.1393\n",
      "Time taken for 1 epoch: 1641.92 secs\n",
      "\n",
      "Epoch 10 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4541 - dense_59_loss: 5.4541\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4678 - dense_59_loss: 5.4678\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2317 - dense_59_loss: 5.2317\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1651 - dense_59_loss: 5.1651\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1386 - dense_59_loss: 5.1386\n",
      "Time taken for 1 epoch: 1609.83 secs\n",
      "\n",
      "Epoch 11 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4530 - dense_59_loss: 5.4530\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4668 - dense_59_loss: 5.4668\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2301 - dense_59_loss: 5.2301\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1636 - dense_59_loss: 5.1636\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1383 - dense_59_loss: 5.1383\n",
      "Time taken for 1 epoch: 1609.83 secs\n",
      "\n",
      "Epoch 12 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4525 - dense_59_loss: 5.4525\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4658 - dense_59_loss: 5.4658\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2303 - dense_59_loss: 5.2303\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1633 - dense_59_loss: 5.1633\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1372 - dense_59_loss: 5.1372\n",
      "Time taken for 1 epoch: 1609.85 secs\n",
      "\n",
      "Epoch 13 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4536 - dense_59_loss: 5.4536\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4650 - dense_59_loss: 5.4650\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2291 - dense_59_loss: 5.2291\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1627 - dense_59_loss: 5.1627\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1364 - dense_59_loss: 5.1364\n",
      "Time taken for 1 epoch: 1609.86 secs\n",
      "\n",
      "Epoch 14 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4512 - dense_59_loss: 5.4512\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4648 - dense_59_loss: 5.4648\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2293 - dense_59_loss: 5.2293\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1619 - dense_59_loss: 5.1619\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1359 - dense_59_loss: 5.1359\n",
      "Time taken for 1 epoch: 1641.93 secs\n",
      "\n",
      "Epoch 15 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4514 - dense_59_loss: 5.4514\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4643 - dense_59_loss: 5.4643\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2286 - dense_59_loss: 5.2286\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1612 - dense_59_loss: 5.1612\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1355 - dense_59_loss: 5.1355\n",
      "Time taken for 1 epoch: 1641.93 secs\n",
      "\n",
      "Epoch 16 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4514 - dense_59_loss: 5.4514\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4634 - dense_59_loss: 5.4634\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2283 - dense_59_loss: 5.2283\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1613 - dense_59_loss: 5.1613\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1347 - dense_59_loss: 5.1347\n",
      "Time taken for 1 epoch: 1641.92 secs\n",
      "\n",
      "Epoch 17 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4494 - dense_59_loss: 5.4494\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4627 - dense_59_loss: 5.4627\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2271 - dense_59_loss: 5.2271\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1604 - dense_59_loss: 5.1604\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1342 - dense_59_loss: 5.1342\n",
      "Time taken for 1 epoch: 1607.79 secs\n",
      "\n",
      "Epoch 18 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4485 - dense_59_loss: 5.4485\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4623 - dense_59_loss: 5.4623\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2282 - dense_59_loss: 5.2282\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1604 - dense_59_loss: 5.1604\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1342 - dense_59_loss: 5.1342\n",
      "Time taken for 1 epoch: 1609.83 secs\n",
      "\n",
      "Epoch 19 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4483 - dense_59_loss: 5.4483\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4623 - dense_59_loss: 5.4623\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2261 - dense_59_loss: 5.2261\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1596 - dense_59_loss: 5.1596\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1338 - dense_59_loss: 5.1338\n",
      "Time taken for 1 epoch: 1641.92 secs\n",
      "\n",
      "Epoch 20 starting...\n",
      "Batch 1 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4489 - dense_59_loss: 5.4489\n",
      "Batch 2 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.4621 - dense_59_loss: 5.4621\n",
      "Batch 3 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.2262 - dense_59_loss: 5.2262\n",
      "Batch 4 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1594 - dense_59_loss: 5.1594\n",
      "Batch 5 starting...\n",
      "256/256 [==============================] - 320s 1s/step - loss: 5.1336 - dense_59_loss: 5.1336\n",
      "Time taken for 1 epoch: 1609.85 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    print(f'Epoch {epoch + 1} starting...')\n",
    "    for (batch, tensor) in enumerate(wine_dataset):\n",
    "        inp, tar = split_input_target(tensor)\n",
    "        print(f'Batch {batch + 1} starting...')\n",
    "        model.fit(inp, tar)\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b834a1",
   "metadata": {
    "id": "1305e698"
   },
   "source": [
    "## Text Generation\n",
    "\n",
    "### Text Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa93c30",
   "metadata": {
    "id": "4aa93c30"
   },
   "outputs": [],
   "source": [
    "def top_k_sample(logits, top_k = 50):\n",
    "    '''Custom Top_k sampler\n",
    "    '''\n",
    "    logits, indices = tf.math.top_k(logits, k=top_k, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    return np.random.choice(indices, p=preds)\n",
    "\n",
    "def text_generation(seed_text, top_k = 50):\n",
    "    '''Text generator based on the above model.\n",
    "    Requires a seed_text to begin predictions.\n",
    "    '''\n",
    "    token_list = wine_tokenizer.texts_to_sequences([seed_text])[0] # Tokenizing seed text\n",
    "    num_tokens_generated = 0\n",
    "    tokens_generated = []\n",
    "    while num_tokens_generated <= (maxlen + 1 - len(token_list)):  # Limit sentence len to 75\n",
    "        pad_len = maxlen + 1 - len(token_list)\n",
    "        sample_index = len(token_list) - 1\n",
    "        if pad_len < 0:\n",
    "            # Check is seed text is too long\n",
    "            x = token_list[:maxlen]\n",
    "            sample_index = maxlen - 1\n",
    "        elif pad_len > 0:\n",
    "            # Pad seed text for predictions\n",
    "            x = token_list + [0] * pad_len\n",
    "        else:\n",
    "            # if seed text is = to max length\n",
    "            x = token_list\n",
    "        x = np.array([x])\n",
    "        y, _ = model.predict(x[0])\n",
    "        sample_token = top_k_sample(y[sample_index].flatten(), top_k)\n",
    "        token_list.append(sample_token)\n",
    "        num_tokens_generated = len(token_list)\n",
    "    txt = wine_tokenizer.sequences_to_texts([token_list])\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88389a33",
   "metadata": {
    "id": "88389a33"
   },
   "outputs": [],
   "source": [
    "def fetch_random_seq():\n",
    "    '''Creates a random number, and then fetches a random starting sequence \n",
    "    based on that number from our original dataset, as well as the rest of\n",
    "    the tasting note\n",
    "    '''\n",
    "    value = random_num_pick()\n",
    "    start_seq = test_table['start_seq'][value].values[0]\n",
    "    end_seq = end_seq = test_table['end_seq'][value].values[0]\n",
    "    return start_seq, end_seq\n",
    "\n",
    "def random_num_pick():\n",
    "    return np.random.randint(len(test_table), size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1cdfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805f5b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1805f5b5",
    "outputId": "0be36b26-fba8-4b12-9c95-b70ce19dfc55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 74) for input KerasTensor(type_spec=TensorSpec(shape=(None, 74), dtype=tf.int32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "Random starting sequence: (made from 40% syrah, 40% grenache and 20% cinsault): Light orange. Aromas of dried cherry\n",
      "Generated  sequence: ['made from 40 syrah 40 grenache and 20 cinsault light orange aromas of dried cherry fruit of but the the and and good 2011 ruby sweet with end cherry long']\n",
      "-----------------\n",
      "Ground truth: and rose are lifted and sharpened by a note of white pepper and pick up a floral quality with aeration. Fleshy, focused and dry, offering bitter orange pith and red fruit flavors and a hint of anise that builds with air. Closes with good energy and thrust, leaving subtle floral and mineral notes behind.\n"
     ]
    }
   ],
   "source": [
    "start_seq, end_seq = fetch_random_seq()\n",
    "generated_seq = text_generation(start_seq)\n",
    "print(f'Random starting sequence: {start_seq}')\n",
    "print(f'Generated  sequence: {generated_seq}')\n",
    "print('-----------------')\n",
    "print(f'Ground truth: {end_seq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b872c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17b872c9",
    "outputId": "6a853e44-b06b-4bef-98a0-90298d17e3df",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aromatic and beautiful but the the the on good the and finishes dark and nose of a berry of offering with a to an by this palate']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_note_1 = text_generation('Aromatic and beautiful')\n",
    "wine_note_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc91bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5fc91bc",
    "outputId": "e13a03b3-8a5f-49ff-c7e6-c9a912ecf50b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lush in wine bright licorice end and offering aromas bright of this the flavors and notes by end finishes flavors the']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_note_2 = text_generation('Lush')\n",
    "wine_note_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95057a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b95057a",
    "outputId": "73aa11bf-2068-4be5-ba9d-6c1171469840"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red wine the with that is of finishes flavors sweet in on the']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_note_3 = text_generation('Red Wine')\n",
    "wine_note_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f11c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b61f11c3",
    "outputId": "22c0e073-9ab8-49eb-bd3b-a9ac1af91d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wine that licorice and and for end and and with offering but tannins in 2011 and']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_note_4 = text_generation('Wine')\n",
    "wine_note_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f41ed9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10f41ed9",
    "outputId": "a9ea44c6-dc27-465e-bd24-1e94e10e5f91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and is but and tannins a is and good palate a an sweet a very the and red tannins and of with']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_note_5 = text_generation('Keyboard')\n",
    "wine_note_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8922a9db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8922a9db",
    "outputId": "8c1feba1-807e-4cb1-99a0-b6050b347e38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is and on bright end berry the with of a of and that of and aromas by']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_note_6 = text_generation('Bukit Pasoh')\n",
    "wine_note_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuuqXc3Ud_CX",
   "metadata": {
    "id": "nuuqXc3Ud_CX"
   },
   "outputs": [],
   "source": [
    "def fetch_seq(value):\n",
    "    return test_table['start_seq'][value].values[0]\n",
    "\n",
    "def random_num_pick():\n",
    "    return np.random.randint(len(test_table), size = 1)\n",
    "\n",
    "def random_text_generator():\n",
    "    pick_value = random_num_pick()\n",
    "    start_seq = fetch_seq(pick_value)\n",
    "\n",
    "    x = text_generation(start_seq)\n",
    "\n",
    "    end_seq = test_table['end_seq'][pick_value].values[0]\n",
    "\n",
    "    print(f'Starter seed: \"{start_seq}\"')\n",
    "    print('-----------------------------')\n",
    "    print(f'Generated text: \"{x}\"')\n",
    "    print(f'Original tasting note end sequence: {end_seq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LoEZVM7ud_FX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoEZVM7ud_FX",
    "outputId": "3636e013-3c8b-40b1-8961-4f75a1c0adc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starter seed: \"The 2020 Château des Places has a pure but slightly one-dimensional nose of black cherries\"\n",
      "-----------------------------\n",
      "Generated text: \"['the 2020 château des places has a pure but slightly one dimensional nose of black cherries sweet finish fruit aromas and and']\"\n",
      "Original tasting note end sequence: and blueberry. The palate is ripe and candied, but again, it lacks complexity and personality. All sweetness, but not enough to counterbalance it.\n"
     ]
    }
   ],
   "source": [
    "random_text_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ov0Ix2p6d_IK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ov0Ix2p6d_IK",
    "outputId": "b9645269-5763-4da1-dae0-9e1f3ec8911b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starter seed: \"The 2013 Cabernet Sauvignon Lewelling Ranch is bright, focused and nuanced, with striking aromatics and\"\n",
      "-----------------------------\n",
      "Generated text: \"['the 2013 cabernet sauvignon lewelling ranch is bright focused and nuanced with striking aromatics and and of good with flavors and floral the very this']\"\n",
      "Original tasting note end sequence: quite a bit of complexity. Understated and classy, with attractive freshness and tons of finesse, the Lewelling Ranch shows the more polished style of the year. Even so, there is plenty of underlying tannic grip.\n"
     ]
    }
   ],
   "source": [
    "random_text_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UI9u0VHjd_K_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UI9u0VHjd_K_",
    "outputId": "86214b4e-f430-48da-a35f-bbd47e42c9c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starter seed: \"Deep purple. Cassis and violet on the nose, along with hints of cocoa torrefaction and\"\n",
      "-----------------------------\n",
      "Generated text: \"['deep purple cassis and violet on the nose along with hints of cocoa torrefaction and and to and with flowers and in bright good finish']\"\n",
      "Original tasting note end sequence: herbs. Sounds acidity lifts the flavors of blackberry syrup, coffee and cocoa, but mounting astringent tannins take over on the moderately persistent finish.\n"
     ]
    }
   ],
   "source": [
    "random_text_generator()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Model 2 - Encoder Block.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
